{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "almqXdO8O-03"
   },
   "source": [
    "# Trainer for Graph Generated Computer Vision Models in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RRsArs2dq-MM",
    "outputId": "516e8a80-4128-4231-dcac-b83e5fca304a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Klx1YtLfPBHL"
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NfGn11dJq-pJ",
    "outputId": "297202c7-9fbc-4451-a75d-83cff429f883"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((x_train[0:8] / 255).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TnNzMSJNnui"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=10, dtype='float32')\n",
    "y_test = to_categorical(y_test, num_classes=10, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwdHh5wiq-Ex"
   },
   "outputs": [],
   "source": [
    "train_dataset_ = tf.data.Dataset.from_tensor_slices(((x_train / 255.).astype(np.float32), y_train))\n",
    "test_dataset_ = tf.data.Dataset.from_tensor_slices(((x_test / 255.).astype(np.float32), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uj07jjDiNDJB",
    "outputId": "90f9cea4-5495-4d07-9157-77fdfb2ef8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((32, 32, 3), (10,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8UYJcHnwoyO"
   },
   "outputs": [],
   "source": [
    "def augment(x,y) -> (tf.Tensor, tf.Tensor):\n",
    "  x = tf.image.random_flip_left_right(x)\n",
    "  x = tf.image.random_brightness(x, 0.05)\n",
    "  x = tf.image.random_contrast(x, 0.7,1.3)\n",
    "  x = tf.image.random_hue(x, 0.08)\n",
    "  x = tf.clip_by_value(x, 0., 1.)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment2(x,y) -> (tf.Tensor, tf.Tensor):\n",
    "#   x = tf.image.resize_images(x, (36,36), align_corners=True)\n",
    "  x = tf.image.per_image_standardization(x)\n",
    "  x = tf.image.random_flip_left_right(x)\n",
    "  x = tf.image.pad_to_bounding_box(x,4, 4, 40,40)\n",
    "  x = tf.image.random_crop(x, (32,32,3))\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x,y):\n",
    "    x = tf.image.per_image_standardization(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJRxzIzbq-zB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 11:09:37.659806 18076 deprecation.py:323] From c:\\users\\holcm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset_.map(augment2,num_parallel_calls=4).shuffle(buffer_size=1000).batch(128).repeat()\n",
    "test_dataset = test_dataset_.map(standardize,num_parallel_calls=4).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e_RY7qX3NFm7",
    "outputId": "2b58711a-d81d-40c8-a8de-d7a8bc383e7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, 32, 32, 3), (?, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdZ4krANPJGS"
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9Rl_vZ_ldm1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Concatenate, BatchNormalization, LeakyReLU, Add, MaxPooling2D, Flatten, Dense, GlobalMaxPooling2D, GlobalAveragePooling2D, Softmax\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6yJ0xRtxJkZ"
   },
   "outputs": [],
   "source": [
    "def build_model_20190717_090720(num_channels):\n",
    "    y0 = Input(shape=(32,32,3))\n",
    "    y1 = Conv2D(1*num_channels, (1,1), padding='same', use_bias=False)(y0)\n",
    "    y2 = BatchNormalization()(y1)\n",
    "    y3 = LeakyReLU()(y2)\n",
    "    y4 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y3)\n",
    "    y5 = BatchNormalization()(y4)\n",
    "    y6 = LeakyReLU()(y5)\n",
    "    y7 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y6)\n",
    "    y8 = BatchNormalization()(y7)\n",
    "    y9 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y8)\n",
    "    y10 = BatchNormalization()(y9)\n",
    "    y11 = LeakyReLU()(y10)\n",
    "    y12 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y11)\n",
    "    y13 = BatchNormalization()(y12)\n",
    "    y14 = LeakyReLU()(y13)\n",
    "    y15 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y14)\n",
    "    y16 = BatchNormalization()(y15)\n",
    "    y17 = LeakyReLU()(y16)\n",
    "    y18 = MaxPooling2D(2,2,padding='same')(y17)\n",
    "    y19 = Conv2D(2*num_channels, (1,1), padding='same', use_bias=False)(y18)\n",
    "    y20 = BatchNormalization()(y19)\n",
    "    y21 = LeakyReLU()(y20)\n",
    "    y22 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y21)\n",
    "    y23 = BatchNormalization()(y22)\n",
    "    y24 = LeakyReLU()(y23)\n",
    "    y25 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y24)\n",
    "    y26 = BatchNormalization()(y25)\n",
    "    y27 = LeakyReLU()(y26)\n",
    "    y28 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y24)\n",
    "    y29 = BatchNormalization()(y28)\n",
    "    y30 = LeakyReLU()(y29)\n",
    "    y31 = Concatenate()([y27, y30])\n",
    "    y32 = Conv2D(4*num_channels, (3,3), padding='same', use_bias=False)(y31)\n",
    "    y33 = BatchNormalization()(y32)\n",
    "    y34 = LeakyReLU()(y33)\n",
    "    y35 = Conv2D(4*num_channels, (3,3), padding='same', use_bias=False)(y34)\n",
    "    y36 = BatchNormalization()(y35)\n",
    "    y37 = LeakyReLU()(y36)\n",
    "    y38 = Add()([y34, y37])\n",
    "    y39 = Conv2D(8*num_channels, (3,3), (2,2), padding='same', use_bias=False)(y38)\n",
    "    y40 = BatchNormalization()(y39)\n",
    "    y41 = LeakyReLU()(y40)\n",
    "    y42 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y41)\n",
    "    y43 = BatchNormalization()(y42)\n",
    "    y44 = LeakyReLU()(y43)\n",
    "    y45 = Conv2D(8*num_channels, (1,1), padding='same', use_bias=False)(y44)\n",
    "    y46 = BatchNormalization()(y45)\n",
    "    y47 = LeakyReLU()(y46)\n",
    "    y48 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y47)\n",
    "    y49 = BatchNormalization()(y48)\n",
    "    y50 = LeakyReLU()(y49)\n",
    "    y51 = Conv2D(8*num_channels, (1,1), padding='same', use_bias=False)(y50)\n",
    "    y52 = BatchNormalization()(y51)\n",
    "    y53 = LeakyReLU()(y52)\n",
    "    y54 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y53)\n",
    "    y55 = BatchNormalization()(y54)\n",
    "    y56 = LeakyReLU()(y55)\n",
    "    y57 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y44)\n",
    "    y58 = BatchNormalization()(y57)\n",
    "    y59 = LeakyReLU()(y58)\n",
    "    y60 = Concatenate()([y56, y59])\n",
    "    y61 = GlobalAveragePooling2D()(y60)\n",
    "    y62 = Flatten()(y61)\n",
    "    y63 = Dense(10)(y62)\n",
    "    y64 =  (y63)\n",
    "    return Model(inputs=y0, outputs=y64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "1j2INp6IxKjs",
    "outputId": "f2c1db48-fb55-49e0-f5d0-5c537dff8e9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 11:09:44.872019 18076 deprecation.py:506] From c:\\users\\holcm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "m = build_model_20190717_090720(24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AfblL2wqxOVC",
    "outputId": "5f6fb1e8-560b-4aea-c618-611f497a8d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 24)   72          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 24)   96          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 32, 32, 24)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 24)   5184        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 24)   96          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 24)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 24)   5184        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 24)   96          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 24)   5184        batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 24)   96          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 32, 32, 24)   0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 24)   5184        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 24)   96          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 32, 32, 24)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 24)   5184        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 24)   96          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 32, 32, 24)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 16, 16, 24)   0           leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 16, 16, 48)   1152        max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 16, 16, 48)   192         conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 16, 16, 48)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 16, 16, 48)   20736       leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 16, 16, 48)   192         conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 16, 16, 48)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 16, 16, 48)   20736       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 16, 16, 48)   20736       leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 16, 16, 48)   192         conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 16, 16, 48)   192         conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 16, 16, 48)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 16, 16, 48)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 16, 16, 96)   0           leaky_re_lu_7[0][0]              \n",
      "                                                                 leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 16, 16, 96)   82944       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 16, 16, 96)   384         conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 16, 16, 96)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 16, 16, 96)   82944       leaky_re_lu_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 16, 16, 96)   384         conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 16, 16, 96)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 16, 16, 96)   0           leaky_re_lu_9[0][0]              \n",
      "                                                                 leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 8, 8, 192)    165888      add[0][0]                        \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 8, 8, 192)    768         conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 8, 8, 192)    0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 8, 8, 192)    331776      leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 8, 8, 192)    768         conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 8, 8, 192)    0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 8, 8, 192)    36864       leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 8, 8, 192)    768         conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 8, 8, 192)    0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 8, 8, 192)    331776      leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 8, 8, 192)    768         conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 8, 8, 192)    0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 8, 8, 192)    36864       leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 8, 8, 192)    768         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 8, 8, 192)    0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 8, 8, 192)    331776      leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 8, 8, 192)    331776      leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 8, 8, 192)    768         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 8, 8, 192)    768         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 8, 8, 192)    0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 8, 8, 192)    0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 384)    0           leaky_re_lu_16[0][0]             \n",
      "                                                                 leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 384)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 384)          0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           3850        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,833,298\n",
      "Trainable params: 1,829,554\n",
      "Non-trainable params: 3,744\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0vIj7ud_M_c3",
    "outputId": "020aca85-b3d0-45fc-e9e3-dd79516979e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(m, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yX4PJJg0cXk"
   },
   "outputs": [],
   "source": [
    "def schedule_fn(epoch):\n",
    "  if epoch < 40:\n",
    "    return 1e-5 + 0.05 * epoch / 40.\n",
    "  elif epoch < 80:\n",
    "    return 0.05 - 0.05 * (epoch-40.) / 40.\n",
    "  else:\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_fn2(epoch):\n",
    "  if epoch < 80:\n",
    "    return 0.1\n",
    "  elif epoch < 120:\n",
    "    return 0.01\n",
    "  else:\n",
    "    return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQS1Mgwh0Yyu"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "lr_schedule_cb = keras.callbacks.LearningRateScheduler(schedule_fn2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_log_file = 'best1b.csv'\n",
    "csv_logger_cb = keras.callbacks.CSVLogger(csv_log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C92W09IvtBpb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 11:10:59.626812 18076 deprecation.py:323] From c:\\users\\holcm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.SGD(lr=0.1, decay=1e-4, momentum=0.9)\n",
    "loss = lambda y_true, y_pred: tf.losses.softmax_cross_entropy(y_true, y_pred)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "m.compile(loss=loss,\n",
    "              optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "p1NaX7RctMQy",
    "outputId": "d36b8091-44b3-4550-8f85-9e3e38f1b298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "390/390 [==============================] - 25s 63ms/step - loss: 1.7988 - acc: 0.3374 - val_loss: 1.7014 - val_acc: 0.3877\n",
      "Epoch 2/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 1.4876 - acc: 0.4577 - val_loss: 2.0609 - val_acc: 0.3714\n",
      "Epoch 3/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 1.2256 - acc: 0.5593 - val_loss: 1.3841 - val_acc: 0.5222\n",
      "Epoch 4/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 1.0439 - acc: 0.6250 - val_loss: 1.0450 - val_acc: 0.6325\n",
      "Epoch 5/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.9186 - acc: 0.6739 - val_loss: 1.4624 - val_acc: 0.5931\n",
      "Epoch 6/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.8249 - acc: 0.7105 - val_loss: 0.8770 - val_acc: 0.7027\n",
      "Epoch 7/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.7559 - acc: 0.7352 - val_loss: 0.8702 - val_acc: 0.6980\n",
      "Epoch 8/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.6950 - acc: 0.7575 - val_loss: 0.7765 - val_acc: 0.7418\n",
      "Epoch 9/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.6562 - acc: 0.7734 - val_loss: 0.8529 - val_acc: 0.7174\n",
      "Epoch 10/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.6113 - acc: 0.7871 - val_loss: 0.9115 - val_acc: 0.7143\n",
      "Epoch 11/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.5906 - acc: 0.7963 - val_loss: 0.6669 - val_acc: 0.7663\n",
      "Epoch 12/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.5625 - acc: 0.8045 - val_loss: 0.7613 - val_acc: 0.7512\n",
      "Epoch 13/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.5288 - acc: 0.8163 - val_loss: 0.6204 - val_acc: 0.7931\n",
      "Epoch 14/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.5139 - acc: 0.8232 - val_loss: 0.6218 - val_acc: 0.7920\n",
      "Epoch 15/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.4969 - acc: 0.8260 - val_loss: 0.6361 - val_acc: 0.7844\n",
      "Epoch 16/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.4782 - acc: 0.8336 - val_loss: 0.5604 - val_acc: 0.8132\n",
      "Epoch 17/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.4579 - acc: 0.8393 - val_loss: 0.6055 - val_acc: 0.7889\n",
      "Epoch 18/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.4420 - acc: 0.8471 - val_loss: 0.6416 - val_acc: 0.7902\n",
      "Epoch 19/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.4302 - acc: 0.8507 - val_loss: 0.5312 - val_acc: 0.8187\n",
      "Epoch 20/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.4152 - acc: 0.8569 - val_loss: 0.5482 - val_acc: 0.8165\n",
      "Epoch 21/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.4069 - acc: 0.8566 - val_loss: 0.5150 - val_acc: 0.8259\n",
      "Epoch 22/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.3932 - acc: 0.8638 - val_loss: 0.5540 - val_acc: 0.8186\n",
      "Epoch 23/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.3837 - acc: 0.8661 - val_loss: 0.5353 - val_acc: 0.8256\n",
      "Epoch 24/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.3693 - acc: 0.8716 - val_loss: 0.5123 - val_acc: 0.8371\n",
      "Epoch 25/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.3622 - acc: 0.8736 - val_loss: 0.5778 - val_acc: 0.8189\n",
      "Epoch 26/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.3508 - acc: 0.8776 - val_loss: 0.4675 - val_acc: 0.8440\n",
      "Epoch 27/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.3412 - acc: 0.8803 - val_loss: 0.4750 - val_acc: 0.8409\n",
      "Epoch 28/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.3338 - acc: 0.8831 - val_loss: 0.5779 - val_acc: 0.8143\n",
      "Epoch 29/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.3243 - acc: 0.8855 - val_loss: 0.4732 - val_acc: 0.8505\n",
      "Epoch 30/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.3148 - acc: 0.8900 - val_loss: 0.4855 - val_acc: 0.8471\n",
      "Epoch 31/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.3096 - acc: 0.8923 - val_loss: 0.4488 - val_acc: 0.8539\n",
      "Epoch 32/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.3019 - acc: 0.8943 - val_loss: 0.4696 - val_acc: 0.8496\n",
      "Epoch 33/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2956 - acc: 0.8958 - val_loss: 0.4734 - val_acc: 0.8509\n",
      "Epoch 34/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2888 - acc: 0.8988 - val_loss: 0.4473 - val_acc: 0.8601\n",
      "Epoch 35/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2827 - acc: 0.9011 - val_loss: 0.4381 - val_acc: 0.8587\n",
      "Epoch 36/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2724 - acc: 0.9043 - val_loss: 0.4901 - val_acc: 0.8466\n",
      "Epoch 37/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2651 - acc: 0.9079 - val_loss: 0.4485 - val_acc: 0.8572\n",
      "Epoch 38/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2605 - acc: 0.9089 - val_loss: 0.5190 - val_acc: 0.8461\n",
      "Epoch 39/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2537 - acc: 0.9102 - val_loss: 0.4802 - val_acc: 0.8532\n",
      "Epoch 40/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2477 - acc: 0.9133 - val_loss: 0.4862 - val_acc: 0.8500\n",
      "Epoch 41/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2430 - acc: 0.9144 - val_loss: 0.4701 - val_acc: 0.8558\n",
      "Epoch 42/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2395 - acc: 0.9149 - val_loss: 0.4372 - val_acc: 0.8666\n",
      "Epoch 43/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2296 - acc: 0.9186 - val_loss: 0.4504 - val_acc: 0.8649\n",
      "Epoch 44/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2287 - acc: 0.9201 - val_loss: 0.4297 - val_acc: 0.8694\n",
      "Epoch 45/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2216 - acc: 0.9211 - val_loss: 0.4461 - val_acc: 0.8634\n",
      "Epoch 46/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2155 - acc: 0.9239 - val_loss: 0.4383 - val_acc: 0.8669\n",
      "Epoch 47/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2138 - acc: 0.9236 - val_loss: 0.4220 - val_acc: 0.8702\n",
      "Epoch 48/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2068 - acc: 0.9257 - val_loss: 0.4243 - val_acc: 0.8726\n",
      "Epoch 49/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.2019 - acc: 0.9293 - val_loss: 0.4385 - val_acc: 0.8699\n",
      "Epoch 50/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1975 - acc: 0.9301 - val_loss: 0.4365 - val_acc: 0.8674\n",
      "Epoch 51/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1914 - acc: 0.9317 - val_loss: 0.4377 - val_acc: 0.8681\n",
      "Epoch 52/160\n",
      "390/390 [==============================] - 19s 47ms/step - loss: 0.1880 - acc: 0.9336 - val_loss: 0.4430 - val_acc: 0.8688\n",
      "Epoch 53/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1827 - acc: 0.9362 - val_loss: 0.4221 - val_acc: 0.8739\n",
      "Epoch 54/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1784 - acc: 0.9372 - val_loss: 0.4372 - val_acc: 0.8767\n",
      "Epoch 55/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1728 - acc: 0.9387 - val_loss: 0.4447 - val_acc: 0.8722\n",
      "Epoch 56/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1734 - acc: 0.9381 - val_loss: 0.5044 - val_acc: 0.8613\n",
      "Epoch 57/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1690 - acc: 0.9396 - val_loss: 0.4336 - val_acc: 0.8731\n",
      "Epoch 58/160\n",
      "390/390 [==============================] - 19s 47ms/step - loss: 0.1655 - acc: 0.9420 - val_loss: 0.4627 - val_acc: 0.8702\n",
      "Epoch 59/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1592 - acc: 0.9430 - val_loss: 0.4783 - val_acc: 0.8678\n",
      "Epoch 60/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1588 - acc: 0.9433 - val_loss: 0.4562 - val_acc: 0.8702\n",
      "Epoch 61/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1519 - acc: 0.9460 - val_loss: 0.4779 - val_acc: 0.8704\n",
      "Epoch 62/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1464 - acc: 0.9473 - val_loss: 0.5128 - val_acc: 0.8607\n",
      "Epoch 63/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1469 - acc: 0.9476 - val_loss: 0.4815 - val_acc: 0.8718\n",
      "Epoch 64/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1422 - acc: 0.9478 - val_loss: 0.4751 - val_acc: 0.8707\n",
      "Epoch 65/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1410 - acc: 0.9497 - val_loss: 0.4930 - val_acc: 0.8686\n",
      "Epoch 66/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1351 - acc: 0.9518 - val_loss: 0.4570 - val_acc: 0.8757\n",
      "Epoch 67/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1364 - acc: 0.9508 - val_loss: 0.4681 - val_acc: 0.8752\n",
      "Epoch 68/160\n",
      "390/390 [==============================] - 19s 47ms/step - loss: 0.1285 - acc: 0.9541 - val_loss: 0.4474 - val_acc: 0.8796\n",
      "Epoch 69/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1258 - acc: 0.9550 - val_loss: 0.4818 - val_acc: 0.8781\n",
      "Epoch 70/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1283 - acc: 0.9535 - val_loss: 0.5070 - val_acc: 0.8668\n",
      "Epoch 71/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1197 - acc: 0.9579 - val_loss: 0.5108 - val_acc: 0.8736\n",
      "Epoch 72/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1203 - acc: 0.9575 - val_loss: 0.4867 - val_acc: 0.8790\n",
      "Epoch 73/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1178 - acc: 0.9577 - val_loss: 0.4953 - val_acc: 0.8765\n",
      "Epoch 74/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1141 - acc: 0.9593 - val_loss: 0.5412 - val_acc: 0.8731\n",
      "Epoch 75/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1112 - acc: 0.9601 - val_loss: 0.4557 - val_acc: 0.8854\n",
      "Epoch 76/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1128 - acc: 0.9594 - val_loss: 0.4745 - val_acc: 0.8802\n",
      "Epoch 77/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1030 - acc: 0.9631 - val_loss: 0.5474 - val_acc: 0.8724\n",
      "Epoch 78/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1066 - acc: 0.9625 - val_loss: 0.4916 - val_acc: 0.8817\n",
      "Epoch 79/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.1002 - acc: 0.9641 - val_loss: 0.4842 - val_acc: 0.8822\n",
      "Epoch 80/160\n",
      "390/390 [==============================] - 19s 47ms/step - loss: 0.0995 - acc: 0.9635 - val_loss: 0.4803 - val_acc: 0.8839\n",
      "Epoch 81/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0791 - acc: 0.9728 - val_loss: 0.3919 - val_acc: 0.8990\n",
      "Epoch 82/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0607 - acc: 0.9795 - val_loss: 0.3898 - val_acc: 0.9002\n",
      "Epoch 83/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0562 - acc: 0.9809 - val_loss: 0.3902 - val_acc: 0.9022\n",
      "Epoch 84/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0544 - acc: 0.9817 - val_loss: 0.3879 - val_acc: 0.9010\n",
      "Epoch 85/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0528 - acc: 0.9824 - val_loss: 0.3935 - val_acc: 0.9014\n",
      "Epoch 86/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0494 - acc: 0.9843 - val_loss: 0.3925 - val_acc: 0.9019\n",
      "Epoch 87/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0486 - acc: 0.9845 - val_loss: 0.3921 - val_acc: 0.9006\n",
      "Epoch 88/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0462 - acc: 0.9851 - val_loss: 0.3938 - val_acc: 0.9018\n",
      "Epoch 89/160\n",
      "390/390 [==============================] - 19s 47ms/step - loss: 0.0469 - acc: 0.9848 - val_loss: 0.3959 - val_acc: 0.9006\n",
      "Epoch 90/160\n",
      "390/390 [==============================] - 19s 50ms/step - loss: 0.0456 - acc: 0.9855 - val_loss: 0.3991 - val_acc: 0.9000\n",
      "Epoch 91/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0451 - acc: 0.9852 - val_loss: 0.3997 - val_acc: 0.9007\n",
      "Epoch 92/160\n",
      "390/390 [==============================] - 19s 49ms/step - loss: 0.0434 - acc: 0.9863 - val_loss: 0.4000 - val_acc: 0.9022\n",
      "Epoch 93/160\n",
      "390/390 [==============================] - 19s 49ms/step - loss: 0.0441 - acc: 0.9855 - val_loss: 0.4043 - val_acc: 0.9012\n",
      "Epoch 94/160\n",
      "390/390 [==============================] - 19s 49ms/step - loss: 0.0438 - acc: 0.9860 - val_loss: 0.4044 - val_acc: 0.9019\n",
      "Epoch 95/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0431 - acc: 0.9862 - val_loss: 0.4098 - val_acc: 0.8999\n",
      "Epoch 96/160\n",
      "390/390 [==============================] - 19s 49ms/step - loss: 0.0424 - acc: 0.9864 - val_loss: 0.4107 - val_acc: 0.9003\n",
      "Epoch 97/160\n",
      "390/390 [==============================] - 19s 49ms/step - loss: 0.0423 - acc: 0.9865 - val_loss: 0.4062 - val_acc: 0.9016\n",
      "Epoch 98/160\n",
      "390/390 [==============================] - 19s 50ms/step - loss: 0.0400 - acc: 0.9871 - val_loss: 0.4101 - val_acc: 0.9016\n",
      "Epoch 99/160\n",
      "390/390 [==============================] - 19s 50ms/step - loss: 0.0391 - acc: 0.9881 - val_loss: 0.4104 - val_acc: 0.9017\n",
      "Epoch 100/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0401 - acc: 0.9874 - val_loss: 0.4112 - val_acc: 0.9008\n",
      "Epoch 101/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0397 - acc: 0.9875 - val_loss: 0.4130 - val_acc: 0.9009\n",
      "Epoch 102/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0402 - acc: 0.9875 - val_loss: 0.4169 - val_acc: 0.9001\n",
      "Epoch 103/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0387 - acc: 0.9879 - val_loss: 0.4189 - val_acc: 0.9001\n",
      "Epoch 104/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0376 - acc: 0.9884 - val_loss: 0.4196 - val_acc: 0.9007\n",
      "Epoch 105/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0372 - acc: 0.9877 - val_loss: 0.4213 - val_acc: 0.9028\n",
      "Epoch 106/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0378 - acc: 0.9871 - val_loss: 0.4197 - val_acc: 0.9011\n",
      "Epoch 107/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0366 - acc: 0.9890 - val_loss: 0.4223 - val_acc: 0.9021\n",
      "Epoch 108/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0371 - acc: 0.9881 - val_loss: 0.4195 - val_acc: 0.9019\n",
      "Epoch 109/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0375 - acc: 0.9881 - val_loss: 0.4218 - val_acc: 0.9014\n",
      "Epoch 110/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0364 - acc: 0.9884 - val_loss: 0.4220 - val_acc: 0.9019\n",
      "Epoch 111/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0347 - acc: 0.9886 - val_loss: 0.4266 - val_acc: 0.9007\n",
      "Epoch 112/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0356 - acc: 0.9888 - val_loss: 0.4243 - val_acc: 0.9023\n",
      "Epoch 113/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0353 - acc: 0.9889 - val_loss: 0.4342 - val_acc: 0.9029\n",
      "Epoch 114/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0350 - acc: 0.9890 - val_loss: 0.4328 - val_acc: 0.9027\n",
      "Epoch 115/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0337 - acc: 0.9900 - val_loss: 0.4309 - val_acc: 0.9029\n",
      "Epoch 116/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0337 - acc: 0.9895 - val_loss: 0.4301 - val_acc: 0.9034\n",
      "Epoch 117/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0352 - acc: 0.9888 - val_loss: 0.4333 - val_acc: 0.9029\n",
      "Epoch 118/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0343 - acc: 0.9896 - val_loss: 0.4345 - val_acc: 0.9020\n",
      "Epoch 119/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0330 - acc: 0.9899 - val_loss: 0.4371 - val_acc: 0.9028\n",
      "Epoch 120/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0335 - acc: 0.9893 - val_loss: 0.4398 - val_acc: 0.9019\n",
      "Epoch 121/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0323 - acc: 0.9905 - val_loss: 0.4391 - val_acc: 0.9017\n",
      "Epoch 122/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0334 - acc: 0.9892 - val_loss: 0.4379 - val_acc: 0.9023\n",
      "Epoch 123/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0319 - acc: 0.9905 - val_loss: 0.4375 - val_acc: 0.9023\n",
      "Epoch 124/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0313 - acc: 0.9902 - val_loss: 0.4379 - val_acc: 0.9022\n",
      "Epoch 125/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0327 - acc: 0.9897 - val_loss: 0.4374 - val_acc: 0.9021\n",
      "Epoch 126/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0321 - acc: 0.9903 - val_loss: 0.4370 - val_acc: 0.9023\n",
      "Epoch 127/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0322 - acc: 0.9900 - val_loss: 0.4367 - val_acc: 0.9025\n",
      "Epoch 128/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0320 - acc: 0.9904 - val_loss: 0.4367 - val_acc: 0.9023\n",
      "Epoch 129/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0309 - acc: 0.9903 - val_loss: 0.4366 - val_acc: 0.9018\n",
      "Epoch 130/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0308 - acc: 0.9907 - val_loss: 0.4369 - val_acc: 0.9025\n",
      "Epoch 131/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0313 - acc: 0.9905 - val_loss: 0.4368 - val_acc: 0.9022\n",
      "Epoch 132/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0311 - acc: 0.9906 - val_loss: 0.4359 - val_acc: 0.9026\n",
      "Epoch 133/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0318 - acc: 0.9906 - val_loss: 0.4373 - val_acc: 0.9017\n",
      "Epoch 134/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0315 - acc: 0.9897 - val_loss: 0.4374 - val_acc: 0.9020\n",
      "Epoch 135/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0314 - acc: 0.9905 - val_loss: 0.4379 - val_acc: 0.9018\n",
      "Epoch 136/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0295 - acc: 0.9908 - val_loss: 0.4376 - val_acc: 0.9017\n",
      "Epoch 137/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0302 - acc: 0.9906 - val_loss: 0.4361 - val_acc: 0.9017\n",
      "Epoch 138/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0307 - acc: 0.9909 - val_loss: 0.4361 - val_acc: 0.9021\n",
      "Epoch 139/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0308 - acc: 0.9903 - val_loss: 0.4358 - val_acc: 0.9027\n",
      "Epoch 140/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0309 - acc: 0.9908 - val_loss: 0.4356 - val_acc: 0.9023\n",
      "Epoch 141/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0306 - acc: 0.9904 - val_loss: 0.4360 - val_acc: 0.9025\n",
      "Epoch 142/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0309 - acc: 0.9909 - val_loss: 0.4362 - val_acc: 0.9025\n",
      "Epoch 143/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0306 - acc: 0.9905 - val_loss: 0.4373 - val_acc: 0.9027\n",
      "Epoch 144/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0314 - acc: 0.9903 - val_loss: 0.4370 - val_acc: 0.9026\n",
      "Epoch 145/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0311 - acc: 0.9900 - val_loss: 0.4378 - val_acc: 0.9022\n",
      "Epoch 146/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0307 - acc: 0.9904 - val_loss: 0.4372 - val_acc: 0.9023\n",
      "Epoch 147/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0304 - acc: 0.9907 - val_loss: 0.4366 - val_acc: 0.9024\n",
      "Epoch 148/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0296 - acc: 0.9914 - val_loss: 0.4374 - val_acc: 0.9022\n",
      "Epoch 149/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0312 - acc: 0.9902 - val_loss: 0.4377 - val_acc: 0.9025\n",
      "Epoch 150/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0299 - acc: 0.9908 - val_loss: 0.4377 - val_acc: 0.9024\n",
      "Epoch 151/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0301 - acc: 0.9903 - val_loss: 0.4373 - val_acc: 0.9026\n",
      "Epoch 152/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0304 - acc: 0.9910 - val_loss: 0.4384 - val_acc: 0.9024\n",
      "Epoch 153/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0300 - acc: 0.9910 - val_loss: 0.4387 - val_acc: 0.9018\n",
      "Epoch 154/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0306 - acc: 0.9905 - val_loss: 0.4384 - val_acc: 0.9029\n",
      "Epoch 155/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0311 - acc: 0.9906 - val_loss: 0.4390 - val_acc: 0.9021\n",
      "Epoch 156/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0288 - acc: 0.9914 - val_loss: 0.4380 - val_acc: 0.9027\n",
      "Epoch 157/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0303 - acc: 0.9911 - val_loss: 0.4388 - val_acc: 0.9023\n",
      "Epoch 158/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0294 - acc: 0.9907 - val_loss: 0.4385 - val_acc: 0.9022\n",
      "Epoch 159/160\n",
      "390/390 [==============================] - 19s 48ms/step - loss: 0.0309 - acc: 0.9902 - val_loss: 0.4394 - val_acc: 0.9024\n",
      "Epoch 160/160\n",
      "390/390 [==============================] - 18s 47ms/step - loss: 0.0300 - acc: 0.9910 - val_loss: 0.4397 - val_acc: 0.9024\n"
     ]
    }
   ],
   "source": [
    "history = m.fit(train_dataset, steps_per_epoch=390, epochs=160, validation_data=test_dataset, callbacks=[lr_schedule_cb, csv_logger_cb], validation_steps=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yO6f7GWdY11E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GGGFC-CIFAR10-Trainer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
