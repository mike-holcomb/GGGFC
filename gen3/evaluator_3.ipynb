{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "almqXdO8O-03"
   },
   "source": [
    "# Trainer for Graph Generated Computer Vision Models in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RRsArs2dq-MM",
    "outputId": "516e8a80-4128-4231-dcac-b83e5fca304a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Klx1YtLfPBHL"
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NfGn11dJq-pJ",
    "outputId": "297202c7-9fbc-4451-a75d-83cff429f883"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((x_train[0:8] / 255).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TnNzMSJNnui"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=10, dtype='float32')\n",
    "y_test = to_categorical(y_test, num_classes=10, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwdHh5wiq-Ex"
   },
   "outputs": [],
   "source": [
    "train_dataset_ = tf.data.Dataset.from_tensor_slices(((x_train / 255.).astype(np.float32), y_train))\n",
    "test_dataset_ = tf.data.Dataset.from_tensor_slices(((x_test / 255.).astype(np.float32), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uj07jjDiNDJB",
    "outputId": "90f9cea4-5495-4d07-9157-77fdfb2ef8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((32, 32, 3), (10,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8UYJcHnwoyO"
   },
   "outputs": [],
   "source": [
    "def augment(x,y) -> (tf.Tensor, tf.Tensor):\n",
    "  x = tf.image.random_flip_left_right(x)\n",
    "  x = tf.image.random_brightness(x, 0.05)\n",
    "  x = tf.image.random_contrast(x, 0.7,1.3)\n",
    "  x = tf.image.random_hue(x, 0.08)\n",
    "  x = tf.clip_by_value(x, 0., 1.)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment2(x,y) -> (tf.Tensor, tf.Tensor):\n",
    "#   x = tf.image.resize_images(x, (36,36), align_corners=True)\n",
    "  x = tf.image.per_image_standardization(x)\n",
    "  x = tf.image.random_flip_left_right(x)\n",
    "  x = tf.image.pad_to_bounding_box(x,4, 4, 40,40)\n",
    "  x = tf.image.random_crop(x, (32,32,3))\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x,y):\n",
    "    x = tf.image.per_image_standardization(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJRxzIzbq-zB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 03:11:24.883376 18136 deprecation.py:323] From c:\\users\\holcm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset_.map(augment2,num_parallel_calls=4).shuffle(buffer_size=1000).batch(128).repeat()\n",
    "test_dataset = test_dataset_.map(standardize,num_parallel_calls=4).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e_RY7qX3NFm7",
    "outputId": "2b58711a-d81d-40c8-a8de-d7a8bc383e7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, 32, 32, 3), (?, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdZ4krANPJGS"
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9Rl_vZ_ldm1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Concatenate, BatchNormalization, LeakyReLU, Add, MaxPooling2D, Flatten, Dense, GlobalMaxPooling2D, GlobalAveragePooling2D, Softmax\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6yJ0xRtxJkZ"
   },
   "outputs": [],
   "source": [
    "def build_model_20190714_112533(num_channels):\n",
    "    y0 = Input(shape=(32,32,3))\n",
    "    y1 = Conv2D(1*num_channels, (1,1), padding='same', use_bias=False)(y0)\n",
    "    y2 = BatchNormalization()(y1)\n",
    "    y3 = LeakyReLU()(y2)\n",
    "    y4 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y3)\n",
    "    y5 = BatchNormalization()(y4)\n",
    "    y6 = LeakyReLU()(y5)\n",
    "    y7 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y6)\n",
    "    y8 = BatchNormalization()(y7)\n",
    "    y9 = LeakyReLU()(y8)\n",
    "    y10 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y6)\n",
    "    y11 = BatchNormalization()(y10)\n",
    "    y12 = LeakyReLU()(y11)\n",
    "    y13 = Concatenate()([y9, y12])\n",
    "    y14 = Conv2D(2*num_channels, (1,1), padding='same', use_bias=False)(y13)\n",
    "    y15 = BatchNormalization()(y14)\n",
    "    y16 = LeakyReLU()(y15)\n",
    "    y17 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y16)\n",
    "    y18 = BatchNormalization()(y17)\n",
    "    y19 = LeakyReLU()(y18)\n",
    "    y20 = Conv2D(2*num_channels, (1,1), padding='same', use_bias=False)(y19)\n",
    "    y21 = BatchNormalization()(y20)\n",
    "    y22 = LeakyReLU()(y21)\n",
    "    y23 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y22)\n",
    "    y24 = BatchNormalization()(y23)\n",
    "    y25 = LeakyReLU()(y24)\n",
    "    y26 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y13)\n",
    "    y27 = BatchNormalization()(y26)\n",
    "    y28 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y27)\n",
    "    y29 = BatchNormalization()(y28)\n",
    "    y30 = LeakyReLU()(y29)\n",
    "    y31 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y30)\n",
    "    y32 = BatchNormalization()(y31)\n",
    "    y33 = LeakyReLU()(y32)\n",
    "    y34 = Add()([y33, y30])\n",
    "    y35 = Conv2D(2*num_channels, (1,1), padding='same', use_bias=False)(y13)\n",
    "    y36 = BatchNormalization()(y35)\n",
    "    y37 = LeakyReLU()(y36)\n",
    "    y38 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y37)\n",
    "    y39 = BatchNormalization()(y38)\n",
    "    y40 = LeakyReLU()(y39)\n",
    "    y41 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y40)\n",
    "    y42 = BatchNormalization()(y41)\n",
    "    y43 = LeakyReLU()(y42)\n",
    "    y44 = Conv2D(2*num_channels, (1,1), padding='same', use_bias=False)(y13)\n",
    "    y45 = BatchNormalization()(y44)\n",
    "    y46 = LeakyReLU()(y45)\n",
    "    y47 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y46)\n",
    "    y48 = BatchNormalization()(y47)\n",
    "    y49 = LeakyReLU()(y48)\n",
    "    y50 = Conv2D(2*num_channels, (1,1), padding='same', use_bias=False)(y49)\n",
    "    y51 = BatchNormalization()(y50)\n",
    "    y52 = LeakyReLU()(y51)\n",
    "    y53 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y52)\n",
    "    y54 = BatchNormalization()(y53)\n",
    "    y55 = LeakyReLU()(y54)\n",
    "    y56 = Concatenate()([y25, y34, y43, y55])\n",
    "    y57 = Conv2D(16*num_channels, (3,3), (2,2), padding='same', use_bias=False)(y56)\n",
    "    y58 = BatchNormalization()(y57)\n",
    "    y59 = LeakyReLU()(y58)\n",
    "    y60 = Conv2D(16*num_channels, (3,3), padding='same', use_bias=False)(y59)\n",
    "    y61 = BatchNormalization()(y60)\n",
    "    y62 = Conv2D(16*num_channels, (3,3), padding='same', use_bias=False)(y61)\n",
    "    y63 = BatchNormalization()(y62)\n",
    "    y64 = LeakyReLU()(y63)\n",
    "    y65 = Conv2D(16*num_channels, (3,3), padding='same', use_bias=False)(y64)\n",
    "    y66 = BatchNormalization()(y65)\n",
    "    y67 = Conv2D(16*num_channels, (3,3), padding='same', use_bias=False)(y66)\n",
    "    y68 = BatchNormalization()(y67)\n",
    "    y69 = LeakyReLU()(y68)\n",
    "    y70 = Conv2D(16*num_channels, (1,1), padding='same', use_bias=False)(y69)\n",
    "    y71 = BatchNormalization()(y70)\n",
    "    y72 = LeakyReLU()(y71)\n",
    "    y73 = Conv2D(16*num_channels, (3,3), padding='same', use_bias=False)(y72)\n",
    "    y74 = BatchNormalization()(y73)\n",
    "    y75 = LeakyReLU()(y74)\n",
    "    y76 = Add()([y69, y75])\n",
    "    y77 = Conv2D(16*num_channels, (1,1), padding='same', use_bias=False)(y64)\n",
    "    y78 = BatchNormalization()(y77)\n",
    "    y79 = LeakyReLU()(y78)\n",
    "    y80 = Conv2D(16*num_channels, (3,3), padding='same', use_bias=False)(y79)\n",
    "    y81 = BatchNormalization()(y80)\n",
    "    y82 = LeakyReLU()(y81)\n",
    "    y83 = Conv2D(16*num_channels, (1,1), padding='same', use_bias=False)(y82)\n",
    "    y84 = BatchNormalization()(y83)\n",
    "    y85 = LeakyReLU()(y84)\n",
    "    y86 = Conv2D(16*num_channels, (3,3), padding='same', use_bias=False)(y85)\n",
    "    y87 = BatchNormalization()(y86)\n",
    "    y88 = LeakyReLU()(y87)\n",
    "    y89 = Add()([y82, y88])\n",
    "    y90 = Concatenate()([y76, y89])\n",
    "    y91 = GlobalAveragePooling2D()(y90)\n",
    "    y92 = Flatten()(y91)\n",
    "    y93 = Dense(10)(y92)\n",
    "    y94 =  (y93)\n",
    "    return Model(inputs=y0, outputs=y94)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "1j2INp6IxKjs",
    "outputId": "f2c1db48-fb55-49e0-f5d0-5c537dff8e9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 03:11:43.560124 18136 deprecation.py:506] From c:\\users\\holcm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "m = build_model_20190714_112533(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AfblL2wqxOVC",
    "outputId": "5f6fb1e8-560b-4aea-c618-611f497a8d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 8)    24          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 8)    32          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 32, 32, 8)    0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 8)    576         leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 8)    32          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 8)    0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 8)    576         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 8)    576         leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 8)    32          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 8)    32          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 32, 32, 8)    0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 32, 32, 8)    0           batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 16)   0           leaky_re_lu_2[0][0]              \n",
      "                                                                 leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   256         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 16)   256         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2304        leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2304        concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 16)   256         concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2304        batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   256         leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 32, 32, 16)   256         leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 32, 32, 16)   64          conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2304        leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 16)   64          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 32, 32, 16)   64          conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           leaky_re_lu_9[0][0]              \n",
      "                                                                 leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 32, 32, 64)   0           leaky_re_lu_7[0][0]              \n",
      "                                                                 add[0][0]                        \n",
      "                                                                 leaky_re_lu_12[0][0]             \n",
      "                                                                 leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 128)  73728       concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 128)  147456      leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 128)  147456      batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_25 (Conv2D)              (None, 16, 16, 128)  16384       leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 16, 16, 128)  147456      leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_25 (BatchNo (None, 16, 16, 128)  512         conv2d_25[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 16, 16, 128)  512         conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_22 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_25[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 16, 16, 128)  147456      batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_26 (Conv2D)              (None, 16, 16, 128)  147456      leaky_re_lu_22[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 16, 16, 128)  512         conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_26 (BatchNo (None, 16, 16, 128)  512         conv2d_26[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_23 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_26[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 16, 16, 128)  16384       leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_27 (Conv2D)              (None, 16, 16, 128)  16384       leaky_re_lu_23[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 16, 16, 128)  512         conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_27 (BatchNo (None, 16, 16, 128)  512         conv2d_27[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_20 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_24 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_27[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 16, 16, 128)  147456      leaky_re_lu_20[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_28 (Conv2D)              (None, 16, 16, 128)  147456      leaky_re_lu_24[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 16, 16, 128)  512         conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_28 (BatchNo (None, 16, 16, 128)  512         conv2d_28[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_21 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_25 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_28[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 16, 16, 128)  0           leaky_re_lu_19[0][0]             \n",
      "                                                                 leaky_re_lu_21[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_2 (Add)                     (None, 16, 16, 128)  0           leaky_re_lu_23[0][0]             \n",
      "                                                                 leaky_re_lu_25[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_2 (Concatenate)     (None, 16, 16, 256)  0           add_1[0][0]                      \n",
      "                                                                 add_2[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 256)          0           concatenate_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 256)          0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           2570        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,188,066\n",
      "Trainable params: 1,184,738\n",
      "Non-trainable params: 3,328\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0vIj7ud_M_c3",
    "outputId": "020aca85-b3d0-45fc-e9e3-dd79516979e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(m, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yX4PJJg0cXk"
   },
   "outputs": [],
   "source": [
    "def schedule_fn(epoch):\n",
    "  if epoch < 40:\n",
    "    return 1e-5 + 0.05 * epoch / 40.\n",
    "  elif epoch < 80:\n",
    "    return 0.05 - 0.05 * (epoch-40.) / 40.\n",
    "  else:\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_fn2(epoch):\n",
    "  if epoch < 80:\n",
    "    return 0.1\n",
    "  elif epoch < 120:\n",
    "    return 0.01\n",
    "  else:\n",
    "    return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQS1Mgwh0Yyu"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "lr_schedule_cb = keras.callbacks.LearningRateScheduler(schedule_fn2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_log_file = 'best3.csv'\n",
    "csv_logger_cb = keras.callbacks.CSVLogger(csv_log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C92W09IvtBpb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 03:12:20.569448 18136 deprecation.py:323] From c:\\users\\holcm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.SGD(lr=0.1, decay=1e-4, momentum=0.9)\n",
    "loss = lambda y_true, y_pred: tf.losses.softmax_cross_entropy(y_true, y_pred)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "m.compile(loss=loss,\n",
    "              optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "p1NaX7RctMQy",
    "outputId": "d36b8091-44b3-4550-8f85-9e3e38f1b298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "390/390 [==============================] - 38s 96ms/step - loss: 1.6618 - acc: 0.3953 - val_loss: 1.4182 - val_acc: 0.5057\n",
      "Epoch 2/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 1.2216 - acc: 0.5656 - val_loss: 1.6229 - val_acc: 0.4840\n",
      "Epoch 3/160\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 1.0269 - acc: 0.6353 - val_loss: 2.0529 - val_acc: 0.4844\n",
      "Epoch 4/160\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 0.9043 - acc: 0.6813 - val_loss: 1.9510 - val_acc: 0.5062\n",
      "Epoch 5/160\n",
      "390/390 [==============================] - 31s 80ms/step - loss: 0.8137 - acc: 0.7147 - val_loss: 0.9934 - val_acc: 0.6816\n",
      "Epoch 6/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.7449 - acc: 0.7392 - val_loss: 1.2798 - val_acc: 0.6483\n",
      "Epoch 7/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.6893 - acc: 0.7596 - val_loss: 0.7824 - val_acc: 0.7383\n",
      "Epoch 8/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.6477 - acc: 0.7751 - val_loss: 0.7171 - val_acc: 0.7558\n",
      "Epoch 9/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.6197 - acc: 0.7853 - val_loss: 0.7598 - val_acc: 0.7552\n",
      "Epoch 10/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.5836 - acc: 0.7966 - val_loss: 0.8260 - val_acc: 0.7300\n",
      "Epoch 11/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.5559 - acc: 0.8071 - val_loss: 0.6417 - val_acc: 0.7949\n",
      "Epoch 12/160\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 0.5369 - acc: 0.8138 - val_loss: 0.7162 - val_acc: 0.7664\n",
      "Epoch 13/160\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 0.5161 - acc: 0.8215 - val_loss: 0.8639 - val_acc: 0.7295\n",
      "Epoch 14/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.4929 - acc: 0.8299 - val_loss: 0.8504 - val_acc: 0.7242\n",
      "Epoch 15/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.4744 - acc: 0.8350 - val_loss: 0.6123 - val_acc: 0.7964\n",
      "Epoch 16/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.4608 - acc: 0.8406 - val_loss: 0.6328 - val_acc: 0.7874\n",
      "Epoch 17/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.4448 - acc: 0.8477 - val_loss: 0.6046 - val_acc: 0.8017\n",
      "Epoch 18/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.4267 - acc: 0.8520 - val_loss: 0.6088 - val_acc: 0.8018\n",
      "Epoch 19/160\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 0.4195 - acc: 0.8556 - val_loss: 0.5919 - val_acc: 0.8105\n",
      "Epoch 20/160\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 0.4013 - acc: 0.8606 - val_loss: 0.6941 - val_acc: 0.7826\n",
      "Epoch 21/160\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 0.3958 - acc: 0.8624 - val_loss: 0.4979 - val_acc: 0.8380\n",
      "Epoch 22/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.3805 - acc: 0.8660 - val_loss: 0.7627 - val_acc: 0.7669\n",
      "Epoch 23/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.3693 - acc: 0.8717 - val_loss: 0.5564 - val_acc: 0.8298\n",
      "Epoch 24/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.3607 - acc: 0.8743 - val_loss: 0.5747 - val_acc: 0.8232\n",
      "Epoch 25/160\n",
      "390/390 [==============================] - 31s 80ms/step - loss: 0.3518 - acc: 0.8771 - val_loss: 0.4851 - val_acc: 0.8413\n",
      "Epoch 26/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.3382 - acc: 0.8823 - val_loss: 0.7409 - val_acc: 0.7852\n",
      "Epoch 27/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.3319 - acc: 0.8851 - val_loss: 0.5785 - val_acc: 0.8143\n",
      "Epoch 28/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.3208 - acc: 0.8898 - val_loss: 0.4864 - val_acc: 0.8439\n",
      "Epoch 29/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.3176 - acc: 0.8912 - val_loss: 0.4525 - val_acc: 0.8559\n",
      "Epoch 30/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.3046 - acc: 0.8933 - val_loss: 0.5923 - val_acc: 0.8249\n",
      "Epoch 31/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.2975 - acc: 0.8967 - val_loss: 0.4328 - val_acc: 0.8559\n",
      "Epoch 32/160\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 0.2896 - acc: 0.8993 - val_loss: 0.5741 - val_acc: 0.8211\n",
      "Epoch 33/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2880 - acc: 0.9001 - val_loss: 0.5013 - val_acc: 0.8429\n",
      "Epoch 34/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2807 - acc: 0.9030 - val_loss: 0.4737 - val_acc: 0.8547\n",
      "Epoch 35/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2729 - acc: 0.9057 - val_loss: 0.4662 - val_acc: 0.8567\n",
      "Epoch 36/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2709 - acc: 0.9039 - val_loss: 0.4475 - val_acc: 0.8586\n",
      "Epoch 37/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2597 - acc: 0.9099 - val_loss: 0.4969 - val_acc: 0.8448\n",
      "Epoch 38/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2529 - acc: 0.9128 - val_loss: 0.4823 - val_acc: 0.8545\n",
      "Epoch 39/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2481 - acc: 0.9138 - val_loss: 0.5275 - val_acc: 0.8428\n",
      "Epoch 40/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2401 - acc: 0.9149 - val_loss: 0.4307 - val_acc: 0.8632\n",
      "Epoch 41/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2361 - acc: 0.9183 - val_loss: 0.4963 - val_acc: 0.8594\n",
      "Epoch 42/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2325 - acc: 0.9189 - val_loss: 0.4271 - val_acc: 0.8698\n",
      "Epoch 43/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2291 - acc: 0.9189 - val_loss: 0.4375 - val_acc: 0.8659\n",
      "Epoch 44/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2248 - acc: 0.9216 - val_loss: 0.5028 - val_acc: 0.8589\n",
      "Epoch 45/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2111 - acc: 0.9266 - val_loss: 0.4651 - val_acc: 0.8591\n",
      "Epoch 46/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2133 - acc: 0.9254 - val_loss: 0.4674 - val_acc: 0.8586\n",
      "Epoch 47/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2060 - acc: 0.9268 - val_loss: 0.4987 - val_acc: 0.8584\n",
      "Epoch 48/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.2035 - acc: 0.9281 - val_loss: 0.4720 - val_acc: 0.8619\n",
      "Epoch 49/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.1984 - acc: 0.9305 - val_loss: 0.4259 - val_acc: 0.8714\n",
      "Epoch 50/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.1925 - acc: 0.9319 - val_loss: 0.4923 - val_acc: 0.8625\n",
      "Epoch 51/160\n",
      "390/390 [==============================] - 31s 79ms/step - loss: 0.1884 - acc: 0.9330 - val_loss: 0.4882 - val_acc: 0.8626\n",
      "Epoch 52/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1836 - acc: 0.9359 - val_loss: 0.4249 - val_acc: 0.8752\n",
      "Epoch 53/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1852 - acc: 0.9354 - val_loss: 0.4543 - val_acc: 0.8688\n",
      "Epoch 54/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1828 - acc: 0.9344 - val_loss: 0.4467 - val_acc: 0.8716\n",
      "Epoch 55/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1715 - acc: 0.9388 - val_loss: 0.5146 - val_acc: 0.8633\n",
      "Epoch 56/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1755 - acc: 0.9369 - val_loss: 0.4801 - val_acc: 0.8645\n",
      "Epoch 57/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1689 - acc: 0.9404 - val_loss: 0.5017 - val_acc: 0.8644\n",
      "Epoch 58/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1677 - acc: 0.9409 - val_loss: 0.5170 - val_acc: 0.8583\n",
      "Epoch 59/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1532 - acc: 0.9454 - val_loss: 0.4794 - val_acc: 0.8702\n",
      "Epoch 60/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1593 - acc: 0.9431 - val_loss: 0.5506 - val_acc: 0.8561\n",
      "Epoch 61/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1544 - acc: 0.9450 - val_loss: 0.5838 - val_acc: 0.8514\n",
      "Epoch 62/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1479 - acc: 0.9469 - val_loss: 0.5586 - val_acc: 0.8514\n",
      "Epoch 63/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1520 - acc: 0.9454 - val_loss: 0.5579 - val_acc: 0.8561\n",
      "Epoch 64/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1438 - acc: 0.9484 - val_loss: 0.4556 - val_acc: 0.8763\n",
      "Epoch 65/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1462 - acc: 0.9488 - val_loss: 0.5035 - val_acc: 0.8707\n",
      "Epoch 66/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1365 - acc: 0.9520 - val_loss: 0.4946 - val_acc: 0.8656\n",
      "Epoch 67/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1385 - acc: 0.9515 - val_loss: 0.5336 - val_acc: 0.8620\n",
      "Epoch 68/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1326 - acc: 0.9522 - val_loss: 0.4198 - val_acc: 0.8854\n",
      "Epoch 69/160\n",
      "390/390 [==============================] - 31s 78ms/step - loss: 0.1289 - acc: 0.9541 - val_loss: 0.6234 - val_acc: 0.8512\n",
      "Epoch 70/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1311 - acc: 0.9529 - val_loss: 0.5595 - val_acc: 0.8647\n",
      "Epoch 71/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1228 - acc: 0.9566 - val_loss: 0.5441 - val_acc: 0.8671\n",
      "Epoch 72/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1227 - acc: 0.9555 - val_loss: 0.5298 - val_acc: 0.8615\n",
      "Epoch 73/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1152 - acc: 0.9594 - val_loss: 0.5821 - val_acc: 0.8606\n",
      "Epoch 74/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1113 - acc: 0.9611 - val_loss: 0.4710 - val_acc: 0.8800\n",
      "Epoch 75/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1153 - acc: 0.9587 - val_loss: 0.4700 - val_acc: 0.8798\n",
      "Epoch 76/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1134 - acc: 0.9591 - val_loss: 0.4759 - val_acc: 0.8799\n",
      "Epoch 77/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1095 - acc: 0.9617 - val_loss: 0.6086 - val_acc: 0.8576\n",
      "Epoch 78/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1102 - acc: 0.9606 - val_loss: 0.5895 - val_acc: 0.8565\n",
      "Epoch 79/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1046 - acc: 0.9624 - val_loss: 0.4978 - val_acc: 0.8739\n",
      "Epoch 80/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.1059 - acc: 0.9625 - val_loss: 0.5112 - val_acc: 0.8773\n",
      "Epoch 81/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0782 - acc: 0.9725 - val_loss: 0.3775 - val_acc: 0.9012\n",
      "Epoch 82/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0631 - acc: 0.9799 - val_loss: 0.3810 - val_acc: 0.8995\n",
      "Epoch 83/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0573 - acc: 0.9820 - val_loss: 0.3838 - val_acc: 0.9002\n",
      "Epoch 84/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0561 - acc: 0.9826 - val_loss: 0.3780 - val_acc: 0.9015\n",
      "Epoch 85/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0539 - acc: 0.9830 - val_loss: 0.3792 - val_acc: 0.9014\n",
      "Epoch 86/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0539 - acc: 0.9832 - val_loss: 0.3796 - val_acc: 0.9026\n",
      "Epoch 87/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0510 - acc: 0.9841 - val_loss: 0.3844 - val_acc: 0.9019\n",
      "Epoch 88/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0498 - acc: 0.9845 - val_loss: 0.3853 - val_acc: 0.9022\n",
      "Epoch 89/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0498 - acc: 0.9849 - val_loss: 0.3836 - val_acc: 0.9018\n",
      "Epoch 90/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0495 - acc: 0.9846 - val_loss: 0.3860 - val_acc: 0.9006\n",
      "Epoch 91/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0478 - acc: 0.9855 - val_loss: 0.3857 - val_acc: 0.9016\n",
      "Epoch 92/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0482 - acc: 0.9850 - val_loss: 0.3847 - val_acc: 0.9028\n",
      "Epoch 93/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0478 - acc: 0.9859 - val_loss: 0.3877 - val_acc: 0.9017\n",
      "Epoch 94/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0458 - acc: 0.9863 - val_loss: 0.3872 - val_acc: 0.9028\n",
      "Epoch 95/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0461 - acc: 0.9861 - val_loss: 0.3893 - val_acc: 0.9047\n",
      "Epoch 96/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0438 - acc: 0.9873 - val_loss: 0.3870 - val_acc: 0.9030\n",
      "Epoch 97/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0445 - acc: 0.9873 - val_loss: 0.3903 - val_acc: 0.9019\n",
      "Epoch 98/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0433 - acc: 0.9875 - val_loss: 0.3888 - val_acc: 0.9027\n",
      "Epoch 99/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0436 - acc: 0.9871 - val_loss: 0.3919 - val_acc: 0.9029\n",
      "Epoch 100/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0433 - acc: 0.9871 - val_loss: 0.3934 - val_acc: 0.9026\n",
      "Epoch 101/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0423 - acc: 0.9873 - val_loss: 0.3942 - val_acc: 0.9019\n",
      "Epoch 102/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0422 - acc: 0.9874 - val_loss: 0.3968 - val_acc: 0.9022\n",
      "Epoch 103/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0402 - acc: 0.9881 - val_loss: 0.3945 - val_acc: 0.9025\n",
      "Epoch 104/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0415 - acc: 0.9877 - val_loss: 0.3954 - val_acc: 0.9013\n",
      "Epoch 105/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0418 - acc: 0.9874 - val_loss: 0.3971 - val_acc: 0.9029\n",
      "Epoch 106/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0389 - acc: 0.9888 - val_loss: 0.3976 - val_acc: 0.9027\n",
      "Epoch 107/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0401 - acc: 0.9885 - val_loss: 0.4009 - val_acc: 0.9022\n",
      "Epoch 108/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0393 - acc: 0.9888 - val_loss: 0.3986 - val_acc: 0.9025\n",
      "Epoch 109/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0401 - acc: 0.9880 - val_loss: 0.3981 - val_acc: 0.9040\n",
      "Epoch 110/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0398 - acc: 0.9884 - val_loss: 0.3984 - val_acc: 0.9042\n",
      "Epoch 111/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0388 - acc: 0.9884 - val_loss: 0.4043 - val_acc: 0.9023\n",
      "Epoch 112/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0386 - acc: 0.9881 - val_loss: 0.4016 - val_acc: 0.9036\n",
      "Epoch 113/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0379 - acc: 0.9888 - val_loss: 0.4001 - val_acc: 0.9016\n",
      "Epoch 114/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0377 - acc: 0.9884 - val_loss: 0.4032 - val_acc: 0.9031\n",
      "Epoch 115/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0385 - acc: 0.9883 - val_loss: 0.4047 - val_acc: 0.9031\n",
      "Epoch 116/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0372 - acc: 0.9889 - val_loss: 0.4007 - val_acc: 0.9051\n",
      "Epoch 117/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0365 - acc: 0.9899 - val_loss: 0.4009 - val_acc: 0.9038\n",
      "Epoch 118/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0373 - acc: 0.9889 - val_loss: 0.4061 - val_acc: 0.9028\n",
      "Epoch 119/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0364 - acc: 0.9894 - val_loss: 0.4069 - val_acc: 0.9035\n",
      "Epoch 120/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0369 - acc: 0.9893 - val_loss: 0.4064 - val_acc: 0.9034\n",
      "Epoch 121/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0353 - acc: 0.9898 - val_loss: 0.4043 - val_acc: 0.9039\n",
      "Epoch 122/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0351 - acc: 0.9895 - val_loss: 0.4035 - val_acc: 0.9038\n",
      "Epoch 123/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0347 - acc: 0.9899 - val_loss: 0.4041 - val_acc: 0.9040\n",
      "Epoch 124/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0348 - acc: 0.9900 - val_loss: 0.4040 - val_acc: 0.9040\n",
      "Epoch 125/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0344 - acc: 0.9901 - val_loss: 0.4035 - val_acc: 0.9043\n",
      "Epoch 126/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0348 - acc: 0.9903 - val_loss: 0.4035 - val_acc: 0.9042\n",
      "Epoch 127/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0345 - acc: 0.9905 - val_loss: 0.4038 - val_acc: 0.9039\n",
      "Epoch 128/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0346 - acc: 0.9906 - val_loss: 0.4029 - val_acc: 0.9041\n",
      "Epoch 129/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0347 - acc: 0.9897 - val_loss: 0.4033 - val_acc: 0.9044\n",
      "Epoch 130/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0332 - acc: 0.9907 - val_loss: 0.4041 - val_acc: 0.9047\n",
      "Epoch 131/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0348 - acc: 0.9900 - val_loss: 0.4040 - val_acc: 0.9045\n",
      "Epoch 132/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0337 - acc: 0.9903 - val_loss: 0.4038 - val_acc: 0.9041\n",
      "Epoch 133/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0343 - acc: 0.9905 - val_loss: 0.4038 - val_acc: 0.9044\n",
      "Epoch 134/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0342 - acc: 0.9898 - val_loss: 0.4039 - val_acc: 0.9041\n",
      "Epoch 135/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0345 - acc: 0.9902 - val_loss: 0.4042 - val_acc: 0.9046\n",
      "Epoch 136/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0332 - acc: 0.9910 - val_loss: 0.4041 - val_acc: 0.9045\n",
      "Epoch 137/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0334 - acc: 0.9907 - val_loss: 0.4047 - val_acc: 0.9042\n",
      "Epoch 138/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0340 - acc: 0.9901 - val_loss: 0.4045 - val_acc: 0.9045\n",
      "Epoch 139/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0341 - acc: 0.9900 - val_loss: 0.4044 - val_acc: 0.9048\n",
      "Epoch 140/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0339 - acc: 0.9909 - val_loss: 0.4043 - val_acc: 0.9043\n",
      "Epoch 141/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0321 - acc: 0.9914 - val_loss: 0.4047 - val_acc: 0.9043\n",
      "Epoch 142/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0349 - acc: 0.9901 - val_loss: 0.4043 - val_acc: 0.9043\n",
      "Epoch 143/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0344 - acc: 0.9903 - val_loss: 0.4049 - val_acc: 0.9043\n",
      "Epoch 144/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0337 - acc: 0.9904 - val_loss: 0.4053 - val_acc: 0.9049\n",
      "Epoch 145/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0330 - acc: 0.9906 - val_loss: 0.4047 - val_acc: 0.9046\n",
      "Epoch 146/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0333 - acc: 0.9911 - val_loss: 0.4047 - val_acc: 0.9045\n",
      "Epoch 147/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0343 - acc: 0.9897 - val_loss: 0.4052 - val_acc: 0.9046\n",
      "Epoch 148/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0347 - acc: 0.9900 - val_loss: 0.4050 - val_acc: 0.9047\n",
      "Epoch 149/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0334 - acc: 0.9907 - val_loss: 0.4054 - val_acc: 0.9047\n",
      "Epoch 150/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0337 - acc: 0.9906 - val_loss: 0.4054 - val_acc: 0.9046\n",
      "Epoch 151/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0335 - acc: 0.9906 - val_loss: 0.4057 - val_acc: 0.9049\n",
      "Epoch 152/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0337 - acc: 0.9902 - val_loss: 0.4057 - val_acc: 0.9044\n",
      "Epoch 153/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0333 - acc: 0.9905 - val_loss: 0.4052 - val_acc: 0.9045\n",
      "Epoch 154/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0345 - acc: 0.9898 - val_loss: 0.4057 - val_acc: 0.9047\n",
      "Epoch 155/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0337 - acc: 0.9903 - val_loss: 0.4057 - val_acc: 0.9047\n",
      "Epoch 156/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0339 - acc: 0.9905 - val_loss: 0.4063 - val_acc: 0.9051\n",
      "Epoch 157/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0326 - acc: 0.9909 - val_loss: 0.4057 - val_acc: 0.9049\n",
      "Epoch 158/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0333 - acc: 0.9910 - val_loss: 0.4052 - val_acc: 0.9052\n",
      "Epoch 159/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0340 - acc: 0.9905 - val_loss: 0.4053 - val_acc: 0.9048\n",
      "Epoch 160/160\n",
      "390/390 [==============================] - 30s 78ms/step - loss: 0.0333 - acc: 0.9912 - val_loss: 0.4064 - val_acc: 0.9050\n"
     ]
    }
   ],
   "source": [
    "history = m.fit(train_dataset, steps_per_epoch=390, epochs=160, validation_data=test_dataset, callbacks=[lr_schedule_cb, csv_logger_cb], validation_steps=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yO6f7GWdY11E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GGGFC-CIFAR10-Trainer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
