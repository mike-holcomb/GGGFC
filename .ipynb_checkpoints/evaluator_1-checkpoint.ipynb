{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "almqXdO8O-03"
   },
   "source": [
    "# Trainer for Graph Generated Computer Vision Models in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RRsArs2dq-MM",
    "outputId": "516e8a80-4128-4231-dcac-b83e5fca304a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Klx1YtLfPBHL"
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NfGn11dJq-pJ",
    "outputId": "297202c7-9fbc-4451-a75d-83cff429f883"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((x_train[0:8] / 255).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TnNzMSJNnui"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=10, dtype='float32')\n",
    "y_test = to_categorical(y_test, num_classes=10, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwdHh5wiq-Ex"
   },
   "outputs": [],
   "source": [
    "train_dataset_ = tf.data.Dataset.from_tensor_slices(((x_train / 255.).astype(np.float32), y_train))\n",
    "test_dataset_ = tf.data.Dataset.from_tensor_slices(((x_test / 255.).astype(np.float32), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uj07jjDiNDJB",
    "outputId": "90f9cea4-5495-4d07-9157-77fdfb2ef8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((32, 32, 3), (10,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8UYJcHnwoyO"
   },
   "outputs": [],
   "source": [
    "def augment(x,y) -> (tf.Tensor, tf.Tensor):\n",
    "  x = tf.image.random_flip_left_right(x)\n",
    "  x = tf.image.random_brightness(x, 0.05)\n",
    "  x = tf.image.random_contrast(x, 0.7,1.3)\n",
    "  x = tf.image.random_hue(x, 0.08)\n",
    "  x = tf.clip_by_value(x, 0., 1.)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment2(x,y) -> (tf.Tensor, tf.Tensor):\n",
    "#   x = tf.image.resize_images(x, (36,36), align_corners=True)\n",
    "  x = tf.image.per_image_standardization(x)\n",
    "  x = tf.image.random_flip_left_right(x)\n",
    "  x = tf.image.pad_to_bounding_box(x,4, 4, 40,40)\n",
    "  x = tf.image.random_crop(x, (32,32,3))\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x,y):\n",
    "    x = tf.image.per_image_standardization(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJRxzIzbq-zB"
   },
   "outputs": [],
   "source": [
    "train_dataset = train_dataset_.map(augment2,num_parallel_calls=4).shuffle(buffer_size=1000).batch(128).repeat()\n",
    "test_dataset = test_dataset_.map(standardize,num_parallel_calls=4).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e_RY7qX3NFm7",
    "outputId": "2b58711a-d81d-40c8-a8de-d7a8bc383e7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, 32, 32, 3), (?, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdZ4krANPJGS"
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9Rl_vZ_ldm1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Concatenate, BatchNormalization, LeakyReLU, Add, MaxPooling2D, Flatten, Dense, GlobalMaxPooling2D, GlobalAveragePooling2D, Softmax\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6yJ0xRtxJkZ"
   },
   "outputs": [],
   "source": [
    "def build_model_20190717_090720(num_channels):\n",
    "    y0 = Input(shape=(32,32,3))\n",
    "    y1 = Conv2D(1*num_channels, (1,1), padding='same', use_bias=False)(y0)\n",
    "    y2 = BatchNormalization()(y1)\n",
    "    y3 = LeakyReLU()(y2)\n",
    "    y4 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y3)\n",
    "    y5 = BatchNormalization()(y4)\n",
    "    y6 = LeakyReLU()(y5)\n",
    "    y7 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y6)\n",
    "    y8 = BatchNormalization()(y7)\n",
    "    y9 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y8)\n",
    "    y10 = BatchNormalization()(y9)\n",
    "    y11 = LeakyReLU()(y10)\n",
    "    y12 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y11)\n",
    "    y13 = BatchNormalization()(y12)\n",
    "    y14 = LeakyReLU()(y13)\n",
    "    y15 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y14)\n",
    "    y16 = BatchNormalization()(y15)\n",
    "    y17 = LeakyReLU()(y16)\n",
    "    y18 = MaxPooling2D(2,2,padding='same')(y17)\n",
    "    y19 = Conv2D(2*num_channels, (1,1), padding='same', use_bias=False)(y18)\n",
    "    y20 = BatchNormalization()(y19)\n",
    "    y21 = LeakyReLU()(y20)\n",
    "    y22 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y21)\n",
    "    y23 = BatchNormalization()(y22)\n",
    "    y24 = LeakyReLU()(y23)\n",
    "    y25 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y24)\n",
    "    y26 = BatchNormalization()(y25)\n",
    "    y27 = LeakyReLU()(y26)\n",
    "    y28 = Conv2D(2*num_channels, (3,3), padding='same', use_bias=False)(y24)\n",
    "    y29 = BatchNormalization()(y28)\n",
    "    y30 = LeakyReLU()(y29)\n",
    "    y31 = Concatenate()([y27, y30])\n",
    "    y32 = Conv2D(4*num_channels, (3,3), padding='same', use_bias=False)(y31)\n",
    "    y33 = BatchNormalization()(y32)\n",
    "    y34 = LeakyReLU()(y33)\n",
    "    y35 = Conv2D(4*num_channels, (3,3), padding='same', use_bias=False)(y34)\n",
    "    y36 = BatchNormalization()(y35)\n",
    "    y37 = LeakyReLU()(y36)\n",
    "    y38 = Add()([y34, y37])\n",
    "    y39 = Conv2D(8*num_channels, (3,3), (2,2), padding='same', use_bias=False)(y38)\n",
    "    y40 = BatchNormalization()(y39)\n",
    "    y41 = LeakyReLU()(y40)\n",
    "    y42 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y41)\n",
    "    y43 = BatchNormalization()(y42)\n",
    "    y44 = LeakyReLU()(y43)\n",
    "    y45 = Conv2D(8*num_channels, (1,1), padding='same', use_bias=False)(y44)\n",
    "    y46 = BatchNormalization()(y45)\n",
    "    y47 = LeakyReLU()(y46)\n",
    "    y48 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y47)\n",
    "    y49 = BatchNormalization()(y48)\n",
    "    y50 = LeakyReLU()(y49)\n",
    "    y51 = Conv2D(8*num_channels, (1,1), padding='same', use_bias=False)(y50)\n",
    "    y52 = BatchNormalization()(y51)\n",
    "    y53 = LeakyReLU()(y52)\n",
    "    y54 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y53)\n",
    "    y55 = BatchNormalization()(y54)\n",
    "    y56 = LeakyReLU()(y55)\n",
    "    y57 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y44)\n",
    "    y58 = BatchNormalization()(y57)\n",
    "    y59 = LeakyReLU()(y58)\n",
    "    y60 = Concatenate()([y56, y59])\n",
    "    y61 = GlobalAveragePooling2D()(y60)\n",
    "    y62 = Flatten()(y61)\n",
    "    y63 = Dense(10)(y62)\n",
    "    y64 =  (y63)\n",
    "    return Model(inputs=y0, outputs=y64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "1j2INp6IxKjs",
    "outputId": "f2c1db48-fb55-49e0-f5d0-5c537dff8e9a"
   },
   "outputs": [],
   "source": [
    "m = build_model_20190717_090720(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AfblL2wqxOVC",
    "outputId": "5f6fb1e8-560b-4aea-c618-611f497a8d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_5 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_76 (Conv2D)              (None, 32, 32, 16)   48          input_5[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_76 (BatchNo (None, 32, 32, 16)   64          conv2d_76[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_72 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_76[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_77 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_72[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_77 (BatchNo (None, 32, 32, 16)   64          conv2d_77[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_73 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_77[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_78 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_73[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_78 (BatchNo (None, 32, 32, 16)   64          conv2d_78[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_79 (Conv2D)              (None, 32, 32, 16)   2304        batch_normalization_78[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_79 (BatchNo (None, 32, 32, 16)   64          conv2d_79[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_74 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_79[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_80 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_74[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_80 (BatchNo (None, 32, 32, 16)   64          conv2d_80[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_75 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_80[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_81 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_75[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_81 (BatchNo (None, 32, 32, 16)   64          conv2d_81[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_76 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_81[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2D)  (None, 16, 16, 16)   0           leaky_re_lu_76[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_82 (Conv2D)              (None, 16, 16, 32)   512         max_pooling2d_4[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_82 (BatchNo (None, 16, 16, 32)   128         conv2d_82[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_77 (LeakyReLU)      (None, 16, 16, 32)   0           batch_normalization_82[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_83 (Conv2D)              (None, 16, 16, 32)   9216        leaky_re_lu_77[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_83 (BatchNo (None, 16, 16, 32)   128         conv2d_83[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_78 (LeakyReLU)      (None, 16, 16, 32)   0           batch_normalization_83[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_84 (Conv2D)              (None, 16, 16, 32)   9216        leaky_re_lu_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_85 (Conv2D)              (None, 16, 16, 32)   9216        leaky_re_lu_78[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_84 (BatchNo (None, 16, 16, 32)   128         conv2d_84[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_85 (BatchNo (None, 16, 16, 32)   128         conv2d_85[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_79 (LeakyReLU)      (None, 16, 16, 32)   0           batch_normalization_84[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_80 (LeakyReLU)      (None, 16, 16, 32)   0           batch_normalization_85[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_8 (Concatenate)     (None, 16, 16, 64)   0           leaky_re_lu_79[0][0]             \n",
      "                                                                 leaky_re_lu_80[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_86 (Conv2D)              (None, 16, 16, 64)   36864       concatenate_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_86 (BatchNo (None, 16, 16, 64)   256         conv2d_86[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_81 (LeakyReLU)      (None, 16, 16, 64)   0           batch_normalization_86[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_87 (Conv2D)              (None, 16, 16, 64)   36864       leaky_re_lu_81[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_87 (BatchNo (None, 16, 16, 64)   256         conv2d_87[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_82 (LeakyReLU)      (None, 16, 16, 64)   0           batch_normalization_87[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 16, 16, 64)   0           leaky_re_lu_81[0][0]             \n",
      "                                                                 leaky_re_lu_82[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_88 (Conv2D)              (None, 8, 8, 128)    73728       add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_88 (BatchNo (None, 8, 8, 128)    512         conv2d_88[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_83 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_88[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_89 (Conv2D)              (None, 8, 8, 128)    147456      leaky_re_lu_83[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_89 (BatchNo (None, 8, 8, 128)    512         conv2d_89[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_84 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_89[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_90 (Conv2D)              (None, 8, 8, 128)    16384       leaky_re_lu_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_90 (BatchNo (None, 8, 8, 128)    512         conv2d_90[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_85 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_90[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_91 (Conv2D)              (None, 8, 8, 128)    147456      leaky_re_lu_85[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_91 (BatchNo (None, 8, 8, 128)    512         conv2d_91[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_86 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_91[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_92 (Conv2D)              (None, 8, 8, 128)    16384       leaky_re_lu_86[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_92 (BatchNo (None, 8, 8, 128)    512         conv2d_92[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_87 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_92[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_93 (Conv2D)              (None, 8, 8, 128)    147456      leaky_re_lu_87[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_94 (Conv2D)              (None, 8, 8, 128)    147456      leaky_re_lu_84[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_93 (BatchNo (None, 8, 8, 128)    512         conv2d_93[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_94 (BatchNo (None, 8, 8, 128)    512         conv2d_94[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_88 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_93[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_89 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_94[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_9 (Concatenate)     (None, 8, 8, 256)    0           leaky_re_lu_88[0][0]             \n",
      "                                                                 leaky_re_lu_89[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_4 (Glo (None, 256)          0           concatenate_9[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_4 (Flatten)             (None, 256)          0           global_average_pooling2d_4[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_4 (Dense)                 (None, 10)           2570        flatten_4[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 817,338\n",
      "Trainable params: 814,842\n",
      "Non-trainable params: 2,496\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0vIj7ud_M_c3",
    "outputId": "020aca85-b3d0-45fc-e9e3-dd79516979e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(m, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yX4PJJg0cXk"
   },
   "outputs": [],
   "source": [
    "def schedule_fn(epoch):\n",
    "  if epoch < 40:\n",
    "    return 1e-5 + 0.05 * epoch / 40.\n",
    "  elif epoch < 80:\n",
    "    return 0.05 - 0.05 * (epoch-40.) / 40.\n",
    "  else:\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_fn2(epoch):\n",
    "  if epoch < 80:\n",
    "    return 0.1\n",
    "  elif epoch < 120:\n",
    "    return 0.01\n",
    "  else:\n",
    "    return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQS1Mgwh0Yyu"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "lr_schedule_cb = keras.callbacks.LearningRateScheduler(schedule_fn2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_log_file = 'best1.csv'\n",
    "csv_logger_cb = keras.callbacks.CSVLogger(csv_log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C92W09IvtBpb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.SGD(lr=0.1, decay=1e-4, momentum=0.9)\n",
    "loss = lambda y_true, y_pred: tf.losses.softmax_cross_entropy(y_true, y_pred)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "m.compile(loss=loss,\n",
    "              optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "p1NaX7RctMQy",
    "outputId": "d36b8091-44b3-4550-8f85-9e3e38f1b298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "390/390 [==============================] - 18s 47ms/step - loss: 1.6432 - acc: 0.3977 - val_loss: 2.6063 - val_acc: 0.3695\n",
      "Epoch 2/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 1.2056 - acc: 0.5675 - val_loss: 1.2935 - val_acc: 0.5649\n",
      "Epoch 3/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 1.0172 - acc: 0.6364 - val_loss: 1.0974 - val_acc: 0.6285\n",
      "Epoch 4/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.8908 - acc: 0.6851 - val_loss: 0.9553 - val_acc: 0.6816\n",
      "Epoch 5/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.7990 - acc: 0.7195 - val_loss: 0.9783 - val_acc: 0.6854\n",
      "Epoch 6/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.7163 - acc: 0.7510 - val_loss: 0.9282 - val_acc: 0.7019\n",
      "Epoch 7/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.6693 - acc: 0.7684 - val_loss: 0.8285 - val_acc: 0.7251\n",
      "Epoch 8/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.6262 - acc: 0.7830 - val_loss: 0.6908 - val_acc: 0.7609\n",
      "Epoch 9/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.5987 - acc: 0.7897 - val_loss: 0.6588 - val_acc: 0.7740\n",
      "Epoch 10/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.5711 - acc: 0.8022 - val_loss: 0.7178 - val_acc: 0.7671\n",
      "Epoch 11/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.5455 - acc: 0.8121 - val_loss: 0.6598 - val_acc: 0.7849\n",
      "Epoch 12/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.5174 - acc: 0.8206 - val_loss: 0.6733 - val_acc: 0.7827\n",
      "Epoch 13/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.5010 - acc: 0.8271 - val_loss: 0.6213 - val_acc: 0.7920\n",
      "Epoch 14/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.4814 - acc: 0.8338 - val_loss: 0.6698 - val_acc: 0.7768\n",
      "Epoch 15/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.4707 - acc: 0.8363 - val_loss: 0.6452 - val_acc: 0.7920\n",
      "Epoch 16/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.4506 - acc: 0.8438 - val_loss: 0.6652 - val_acc: 0.7877\n",
      "Epoch 17/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.4400 - acc: 0.8465 - val_loss: 0.5709 - val_acc: 0.8093\n",
      "Epoch 18/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.4261 - acc: 0.8523 - val_loss: 0.6170 - val_acc: 0.8041\n",
      "Epoch 19/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.4095 - acc: 0.8573 - val_loss: 0.6824 - val_acc: 0.7807\n",
      "Epoch 20/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.4019 - acc: 0.8612 - val_loss: 0.6147 - val_acc: 0.7969\n",
      "Epoch 21/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3899 - acc: 0.8645 - val_loss: 0.5119 - val_acc: 0.8342\n",
      "Epoch 22/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3842 - acc: 0.8657 - val_loss: 0.4915 - val_acc: 0.8399\n",
      "Epoch 23/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3670 - acc: 0.8720 - val_loss: 0.5894 - val_acc: 0.8163\n",
      "Epoch 24/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3631 - acc: 0.8717 - val_loss: 0.5049 - val_acc: 0.8326\n",
      "Epoch 25/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3561 - acc: 0.8763 - val_loss: 0.5447 - val_acc: 0.8236\n",
      "Epoch 26/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3441 - acc: 0.8791 - val_loss: 0.5162 - val_acc: 0.8336\n",
      "Epoch 27/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3389 - acc: 0.8821 - val_loss: 0.5185 - val_acc: 0.8356\n",
      "Epoch 28/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3245 - acc: 0.8866 - val_loss: 0.4808 - val_acc: 0.8452\n",
      "Epoch 29/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3182 - acc: 0.8882 - val_loss: 0.4598 - val_acc: 0.8507\n",
      "Epoch 30/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3182 - acc: 0.8873 - val_loss: 0.4284 - val_acc: 0.8613\n",
      "Epoch 31/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3090 - acc: 0.8933 - val_loss: 0.4572 - val_acc: 0.8538\n",
      "Epoch 32/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.3021 - acc: 0.8940 - val_loss: 0.5428 - val_acc: 0.8368\n",
      "Epoch 33/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2963 - acc: 0.8957 - val_loss: 0.4588 - val_acc: 0.8573\n",
      "Epoch 34/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2878 - acc: 0.8987 - val_loss: 0.4210 - val_acc: 0.8651\n",
      "Epoch 35/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2800 - acc: 0.9011 - val_loss: 0.4533 - val_acc: 0.8625\n",
      "Epoch 36/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2742 - acc: 0.9029 - val_loss: 0.5267 - val_acc: 0.8406\n",
      "Epoch 37/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2693 - acc: 0.9061 - val_loss: 0.4738 - val_acc: 0.8562\n",
      "Epoch 38/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2633 - acc: 0.9073 - val_loss: 0.4861 - val_acc: 0.8523\n",
      "Epoch 39/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2615 - acc: 0.9076 - val_loss: 0.4630 - val_acc: 0.8583\n",
      "Epoch 40/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2529 - acc: 0.9116 - val_loss: 0.4833 - val_acc: 0.8536\n",
      "Epoch 41/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2520 - acc: 0.9104 - val_loss: 0.4227 - val_acc: 0.8711\n",
      "Epoch 42/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2455 - acc: 0.9134 - val_loss: 0.4477 - val_acc: 0.8669\n",
      "Epoch 43/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.2402 - acc: 0.9147 - val_loss: 0.4386 - val_acc: 0.8639\n",
      "Epoch 44/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2320 - acc: 0.9177 - val_loss: 0.4432 - val_acc: 0.8674\n",
      "Epoch 45/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2313 - acc: 0.9177 - val_loss: 0.4756 - val_acc: 0.8596\n",
      "Epoch 46/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.2248 - acc: 0.9195 - val_loss: 0.4631 - val_acc: 0.8641\n",
      "Epoch 47/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.2235 - acc: 0.9212 - val_loss: 0.4381 - val_acc: 0.8688\n",
      "Epoch 48/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.2216 - acc: 0.9202 - val_loss: 0.4224 - val_acc: 0.8734\n",
      "Epoch 49/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.2141 - acc: 0.9245 - val_loss: 0.4074 - val_acc: 0.8779\n",
      "Epoch 50/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.2070 - acc: 0.9267 - val_loss: 0.4443 - val_acc: 0.8711\n",
      "Epoch 51/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.2075 - acc: 0.9266 - val_loss: 0.4182 - val_acc: 0.8774\n",
      "Epoch 52/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.2055 - acc: 0.9276 - val_loss: 0.4477 - val_acc: 0.8706\n",
      "Epoch 53/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1984 - acc: 0.9302 - val_loss: 0.4524 - val_acc: 0.8677\n",
      "Epoch 54/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1965 - acc: 0.9297 - val_loss: 0.4845 - val_acc: 0.8682\n",
      "Epoch 55/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.1902 - acc: 0.9320 - val_loss: 0.4333 - val_acc: 0.8701\n",
      "Epoch 56/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1918 - acc: 0.9316 - val_loss: 0.4634 - val_acc: 0.8716\n",
      "Epoch 57/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1890 - acc: 0.9335 - val_loss: 0.4646 - val_acc: 0.8726\n",
      "Epoch 58/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1817 - acc: 0.9359 - val_loss: 0.4353 - val_acc: 0.8755\n",
      "Epoch 59/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1822 - acc: 0.9341 - val_loss: 0.4428 - val_acc: 0.8744\n",
      "Epoch 60/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1749 - acc: 0.9366 - val_loss: 0.4271 - val_acc: 0.8795\n",
      "Epoch 61/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1722 - acc: 0.9382 - val_loss: 0.4314 - val_acc: 0.8774\n",
      "Epoch 62/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.1665 - acc: 0.9404 - val_loss: 0.4379 - val_acc: 0.8779\n",
      "Epoch 63/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.1641 - acc: 0.9417 - val_loss: 0.4312 - val_acc: 0.8763\n",
      "Epoch 64/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.1691 - acc: 0.9397 - val_loss: 0.4567 - val_acc: 0.8706\n",
      "Epoch 65/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.1621 - acc: 0.9414 - val_loss: 0.4306 - val_acc: 0.8805\n",
      "Epoch 66/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1608 - acc: 0.9424 - val_loss: 0.4239 - val_acc: 0.8774\n",
      "Epoch 67/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1528 - acc: 0.9452 - val_loss: 0.4138 - val_acc: 0.8791\n",
      "Epoch 68/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1537 - acc: 0.9449 - val_loss: 0.4406 - val_acc: 0.8815\n",
      "Epoch 69/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.1524 - acc: 0.9452 - val_loss: 0.4730 - val_acc: 0.8718\n",
      "Epoch 70/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.1547 - acc: 0.9433 - val_loss: 0.4943 - val_acc: 0.8695\n",
      "Epoch 71/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.1487 - acc: 0.9470 - val_loss: 0.4585 - val_acc: 0.8750\n",
      "Epoch 72/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.1419 - acc: 0.9485 - val_loss: 0.4349 - val_acc: 0.8824\n",
      "Epoch 73/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.1379 - acc: 0.9503 - val_loss: 0.4922 - val_acc: 0.8735\n",
      "Epoch 74/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1343 - acc: 0.9511 - val_loss: 0.4534 - val_acc: 0.8827\n",
      "Epoch 75/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1374 - acc: 0.9491 - val_loss: 0.4900 - val_acc: 0.8763\n",
      "Epoch 76/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1364 - acc: 0.9510 - val_loss: 0.4728 - val_acc: 0.8792\n",
      "Epoch 77/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1329 - acc: 0.9508 - val_loss: 0.4361 - val_acc: 0.8865\n",
      "Epoch 78/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1284 - acc: 0.9545 - val_loss: 0.4670 - val_acc: 0.8784\n",
      "Epoch 79/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1285 - acc: 0.9534 - val_loss: 0.4683 - val_acc: 0.8834\n",
      "Epoch 80/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1305 - acc: 0.9528 - val_loss: 0.4922 - val_acc: 0.8750\n",
      "Epoch 81/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.1056 - acc: 0.9620 - val_loss: 0.3724 - val_acc: 0.8987\n",
      "Epoch 82/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0863 - acc: 0.9699 - val_loss: 0.3698 - val_acc: 0.8998\n",
      "Epoch 83/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0801 - acc: 0.9725 - val_loss: 0.3705 - val_acc: 0.9009\n",
      "Epoch 84/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0789 - acc: 0.9728 - val_loss: 0.3752 - val_acc: 0.8996\n",
      "Epoch 85/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0785 - acc: 0.9739 - val_loss: 0.3758 - val_acc: 0.8998\n",
      "Epoch 86/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0763 - acc: 0.9741 - val_loss: 0.3762 - val_acc: 0.8982\n",
      "Epoch 87/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0739 - acc: 0.9749 - val_loss: 0.3815 - val_acc: 0.9005\n",
      "Epoch 88/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0715 - acc: 0.9755 - val_loss: 0.3771 - val_acc: 0.9011\n",
      "Epoch 89/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0724 - acc: 0.9754 - val_loss: 0.3788 - val_acc: 0.9003\n",
      "Epoch 90/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0719 - acc: 0.9752 - val_loss: 0.3819 - val_acc: 0.9008\n",
      "Epoch 91/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0714 - acc: 0.9758 - val_loss: 0.3816 - val_acc: 0.9008\n",
      "Epoch 92/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0703 - acc: 0.9756 - val_loss: 0.3817 - val_acc: 0.9009\n",
      "Epoch 93/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0683 - acc: 0.9770 - val_loss: 0.3843 - val_acc: 0.9018\n",
      "Epoch 94/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0672 - acc: 0.9777 - val_loss: 0.3876 - val_acc: 0.8997\n",
      "Epoch 95/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0691 - acc: 0.9768 - val_loss: 0.3879 - val_acc: 0.9010\n",
      "Epoch 96/160\n",
      "390/390 [==============================] - 15s 37ms/step - loss: 0.0683 - acc: 0.9771 - val_loss: 0.3902 - val_acc: 0.8998\n",
      "Epoch 97/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0665 - acc: 0.9780 - val_loss: 0.3938 - val_acc: 0.9000\n",
      "Epoch 98/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0657 - acc: 0.9783 - val_loss: 0.3918 - val_acc: 0.9001\n",
      "Epoch 99/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0650 - acc: 0.9784 - val_loss: 0.3933 - val_acc: 0.9002\n",
      "Epoch 100/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0661 - acc: 0.9783 - val_loss: 0.3929 - val_acc: 0.9001\n",
      "Epoch 101/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0662 - acc: 0.9773 - val_loss: 0.3949 - val_acc: 0.8993\n",
      "Epoch 102/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0644 - acc: 0.9783 - val_loss: 0.3949 - val_acc: 0.9003\n",
      "Epoch 103/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0637 - acc: 0.9780 - val_loss: 0.3945 - val_acc: 0.8993\n",
      "Epoch 104/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0624 - acc: 0.9788 - val_loss: 0.3965 - val_acc: 0.8993\n",
      "Epoch 105/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0616 - acc: 0.9792 - val_loss: 0.3971 - val_acc: 0.9008\n",
      "Epoch 106/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0614 - acc: 0.9794 - val_loss: 0.3973 - val_acc: 0.9011\n",
      "Epoch 107/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0609 - acc: 0.9790 - val_loss: 0.3975 - val_acc: 0.8999\n",
      "Epoch 108/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0606 - acc: 0.9790 - val_loss: 0.3994 - val_acc: 0.9007\n",
      "Epoch 109/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0606 - acc: 0.9801 - val_loss: 0.4032 - val_acc: 0.9008\n",
      "Epoch 110/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0614 - acc: 0.9790 - val_loss: 0.4043 - val_acc: 0.9006\n",
      "Epoch 111/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0593 - acc: 0.9797 - val_loss: 0.4053 - val_acc: 0.9001\n",
      "Epoch 112/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0598 - acc: 0.9793 - val_loss: 0.4053 - val_acc: 0.9006\n",
      "Epoch 113/160\n",
      "390/390 [==============================] - 14s 37ms/step - loss: 0.0579 - acc: 0.9813 - val_loss: 0.4028 - val_acc: 0.8997\n",
      "Epoch 114/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0597 - acc: 0.9798 - val_loss: 0.4043 - val_acc: 0.9006\n",
      "Epoch 115/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0596 - acc: 0.9804 - val_loss: 0.4054 - val_acc: 0.9004\n",
      "Epoch 116/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0583 - acc: 0.9803 - val_loss: 0.4096 - val_acc: 0.9003\n",
      "Epoch 117/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0591 - acc: 0.9799 - val_loss: 0.4060 - val_acc: 0.9014\n",
      "Epoch 118/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0588 - acc: 0.9801 - val_loss: 0.4098 - val_acc: 0.9010\n",
      "Epoch 119/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0552 - acc: 0.9818 - val_loss: 0.4109 - val_acc: 0.9005\n",
      "Epoch 120/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0566 - acc: 0.9813 - val_loss: 0.4130 - val_acc: 0.9011\n",
      "Epoch 121/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0567 - acc: 0.9810 - val_loss: 0.4096 - val_acc: 0.9016\n",
      "Epoch 122/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0535 - acc: 0.9822 - val_loss: 0.4098 - val_acc: 0.9010\n",
      "Epoch 123/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0536 - acc: 0.9824 - val_loss: 0.4093 - val_acc: 0.9008\n",
      "Epoch 124/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0535 - acc: 0.9820 - val_loss: 0.4098 - val_acc: 0.9008\n",
      "Epoch 125/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0578 - acc: 0.9810 - val_loss: 0.4100 - val_acc: 0.8996\n",
      "Epoch 126/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0543 - acc: 0.9817 - val_loss: 0.4100 - val_acc: 0.9002\n",
      "Epoch 127/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0524 - acc: 0.9828 - val_loss: 0.4095 - val_acc: 0.9004\n",
      "Epoch 128/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0543 - acc: 0.9815 - val_loss: 0.4100 - val_acc: 0.9007\n",
      "Epoch 129/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0549 - acc: 0.9807 - val_loss: 0.4095 - val_acc: 0.9007\n",
      "Epoch 130/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0536 - acc: 0.9818 - val_loss: 0.4091 - val_acc: 0.9004\n",
      "Epoch 131/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0558 - acc: 0.9814 - val_loss: 0.4098 - val_acc: 0.9003\n",
      "Epoch 132/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0525 - acc: 0.9821 - val_loss: 0.4091 - val_acc: 0.9004\n",
      "Epoch 133/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0535 - acc: 0.9825 - val_loss: 0.4093 - val_acc: 0.9006\n",
      "Epoch 134/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0536 - acc: 0.9816 - val_loss: 0.4094 - val_acc: 0.9005\n",
      "Epoch 135/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0545 - acc: 0.9820 - val_loss: 0.4097 - val_acc: 0.9012\n",
      "Epoch 136/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0538 - acc: 0.9819 - val_loss: 0.4095 - val_acc: 0.9004\n",
      "Epoch 137/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0549 - acc: 0.9816 - val_loss: 0.4096 - val_acc: 0.9006\n",
      "Epoch 138/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0534 - acc: 0.9823 - val_loss: 0.4096 - val_acc: 0.9003\n",
      "Epoch 139/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0548 - acc: 0.9812 - val_loss: 0.4098 - val_acc: 0.9005\n",
      "Epoch 140/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0537 - acc: 0.9813 - val_loss: 0.4097 - val_acc: 0.9009\n",
      "Epoch 141/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0533 - acc: 0.9819 - val_loss: 0.4096 - val_acc: 0.9010\n",
      "Epoch 142/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0530 - acc: 0.9817 - val_loss: 0.4095 - val_acc: 0.9004\n",
      "Epoch 143/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0534 - acc: 0.9821 - val_loss: 0.4099 - val_acc: 0.9012\n",
      "Epoch 144/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0551 - acc: 0.9809 - val_loss: 0.4096 - val_acc: 0.9010\n",
      "Epoch 145/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0539 - acc: 0.9823 - val_loss: 0.4103 - val_acc: 0.9011\n",
      "Epoch 146/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0542 - acc: 0.9817 - val_loss: 0.4097 - val_acc: 0.9006\n",
      "Epoch 147/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0524 - acc: 0.9816 - val_loss: 0.4102 - val_acc: 0.9008\n",
      "Epoch 148/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0533 - acc: 0.9822 - val_loss: 0.4103 - val_acc: 0.9009\n",
      "Epoch 149/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0537 - acc: 0.9819 - val_loss: 0.4102 - val_acc: 0.9009\n",
      "Epoch 150/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0540 - acc: 0.9825 - val_loss: 0.4112 - val_acc: 0.9010\n",
      "Epoch 151/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0536 - acc: 0.9819 - val_loss: 0.4103 - val_acc: 0.9005\n",
      "Epoch 152/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0516 - acc: 0.9832 - val_loss: 0.4108 - val_acc: 0.9007\n",
      "Epoch 153/160\n",
      "390/390 [==============================] - 14s 35ms/step - loss: 0.0536 - acc: 0.9822 - val_loss: 0.4112 - val_acc: 0.9005\n",
      "Epoch 154/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0527 - acc: 0.9824 - val_loss: 0.4116 - val_acc: 0.9003\n",
      "Epoch 155/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0544 - acc: 0.9818 - val_loss: 0.4108 - val_acc: 0.9016\n",
      "Epoch 156/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0521 - acc: 0.9823 - val_loss: 0.4109 - val_acc: 0.9008\n",
      "Epoch 157/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0534 - acc: 0.9825 - val_loss: 0.4105 - val_acc: 0.9015\n",
      "Epoch 158/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0541 - acc: 0.9811 - val_loss: 0.4109 - val_acc: 0.9011\n",
      "Epoch 159/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0520 - acc: 0.9825 - val_loss: 0.4103 - val_acc: 0.9020\n",
      "Epoch 160/160\n",
      "390/390 [==============================] - 14s 36ms/step - loss: 0.0537 - acc: 0.9824 - val_loss: 0.4105 - val_acc: 0.9015\n"
     ]
    }
   ],
   "source": [
    "history = m.fit(train_dataset, steps_per_epoch=390, epochs=160, validation_data=test_dataset, callbacks=[lr_schedule_cb, csv_logger_cb], validation_steps=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yO6f7GWdY11E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GGGFC-CIFAR10-Trainer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
