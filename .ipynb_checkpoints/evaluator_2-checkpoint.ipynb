{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "almqXdO8O-03"
   },
   "source": [
    "# Trainer for Graph Generated Computer Vision Models in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RRsArs2dq-MM",
    "outputId": "516e8a80-4128-4231-dcac-b83e5fca304a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Klx1YtLfPBHL"
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NfGn11dJq-pJ",
    "outputId": "297202c7-9fbc-4451-a75d-83cff429f883"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((x_train[0:8] / 255).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TnNzMSJNnui"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=10, dtype='float32')\n",
    "y_test = to_categorical(y_test, num_classes=10, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwdHh5wiq-Ex"
   },
   "outputs": [],
   "source": [
    "train_dataset_ = tf.data.Dataset.from_tensor_slices(((x_train / 255.).astype(np.float32), y_train))\n",
    "test_dataset_ = tf.data.Dataset.from_tensor_slices(((x_test / 255.).astype(np.float32), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uj07jjDiNDJB",
    "outputId": "90f9cea4-5495-4d07-9157-77fdfb2ef8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((32, 32, 3), (10,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8UYJcHnwoyO"
   },
   "outputs": [],
   "source": [
    "def augment(x,y) -> (tf.Tensor, tf.Tensor):\n",
    "  x = tf.image.random_flip_left_right(x)\n",
    "  x = tf.image.random_brightness(x, 0.05)\n",
    "  x = tf.image.random_contrast(x, 0.7,1.3)\n",
    "  x = tf.image.random_hue(x, 0.08)\n",
    "  x = tf.clip_by_value(x, 0., 1.)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment2(x,y) -> (tf.Tensor, tf.Tensor):\n",
    "#   x = tf.image.resize_images(x, (36,36), align_corners=True)\n",
    "  x = tf.image.per_image_standardization(x)\n",
    "  x = tf.image.random_flip_left_right(x)\n",
    "  x = tf.image.pad_to_bounding_box(x,4, 4, 40,40)\n",
    "  x = tf.image.random_crop(x, (32,32,3))\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x,y):\n",
    "    x = tf.image.per_image_standardization(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJRxzIzbq-zB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 02:17:20.749334  3712 deprecation.py:323] From c:\\users\\holcm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset_.map(augment2,num_parallel_calls=4).shuffle(buffer_size=1000).batch(128).repeat()\n",
    "test_dataset = test_dataset_.map(standardize,num_parallel_calls=4).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e_RY7qX3NFm7",
    "outputId": "2b58711a-d81d-40c8-a8de-d7a8bc383e7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, 32, 32, 3), (?, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdZ4krANPJGS"
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9Rl_vZ_ldm1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Concatenate, BatchNormalization, LeakyReLU, Add, MaxPooling2D, Flatten, Dense, GlobalMaxPooling2D, GlobalAveragePooling2D, Softmax\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6yJ0xRtxJkZ"
   },
   "outputs": [],
   "source": [
    "def build_model_20190712_010024(num_channels):\n",
    "    y0 = Input(shape=(32,32,3))\n",
    "    y1 = Conv2D(1*num_channels, (1,1), padding='same', use_bias=False)(y0)\n",
    "    y2 = BatchNormalization()(y1)\n",
    "    y3 = LeakyReLU()(y2)\n",
    "    y4 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y3)\n",
    "    y5 = BatchNormalization()(y4)\n",
    "    y6 = LeakyReLU()(y5)\n",
    "    y7 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y6)\n",
    "    y8 = BatchNormalization()(y7)\n",
    "    y9 = LeakyReLU()(y8)\n",
    "    y10 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y9)\n",
    "    y11 = BatchNormalization()(y10)\n",
    "    y12 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y11)\n",
    "    y13 = BatchNormalization()(y12)\n",
    "    y14 = LeakyReLU()(y13)\n",
    "    y15 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y14)\n",
    "    y16 = BatchNormalization()(y15)\n",
    "    y17 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y16)\n",
    "    y18 = BatchNormalization()(y17)\n",
    "    y19 = LeakyReLU()(y18)\n",
    "    y20 = Add()([y14, y19])\n",
    "    y21 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y9)\n",
    "    y22 = BatchNormalization()(y21)\n",
    "    y23 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y22)\n",
    "    y24 = BatchNormalization()(y23)\n",
    "    y25 = LeakyReLU()(y24)\n",
    "    y26 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y25)\n",
    "    y27 = BatchNormalization()(y26)\n",
    "    y28 = LeakyReLU()(y27)\n",
    "    y29 = Add()([y25, y28])\n",
    "    y30 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y9)\n",
    "    y31 = BatchNormalization()(y30)\n",
    "    y32 = LeakyReLU()(y31)\n",
    "    y33 = Conv2D(1*num_channels, (1,1), padding='same', use_bias=False)(y32)\n",
    "    y34 = BatchNormalization()(y33)\n",
    "    y35 = LeakyReLU()(y34)\n",
    "    y36 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y35)\n",
    "    y37 = BatchNormalization()(y36)\n",
    "    y38 = LeakyReLU()(y37)\n",
    "    y39 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y9)\n",
    "    y40 = BatchNormalization()(y39)\n",
    "    y41 = LeakyReLU()(y40)\n",
    "    y42 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y41)\n",
    "    y43 = BatchNormalization()(y42)\n",
    "    y44 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y43)\n",
    "    y45 = BatchNormalization()(y44)\n",
    "    y46 = LeakyReLU()(y45)\n",
    "    y47 = Concatenate()([y20, y29, y38, y46])\n",
    "    y48 = Conv2D(8*num_channels, (3,3), (2,2), padding='same', use_bias=False)(y47)\n",
    "    y49 = BatchNormalization()(y48)\n",
    "    y50 = LeakyReLU()(y49)\n",
    "    y51 = Conv2D(8*num_channels, (1,1), padding='same', use_bias=False)(y50)\n",
    "    y52 = BatchNormalization()(y51)\n",
    "    y53 = LeakyReLU()(y52)\n",
    "    y54 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y53)\n",
    "    y55 = BatchNormalization()(y54)\n",
    "    y56 = LeakyReLU()(y55)\n",
    "    y57 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y56)\n",
    "    y58 = BatchNormalization()(y57)\n",
    "    y59 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y58)\n",
    "    y60 = BatchNormalization()(y59)\n",
    "    y61 = LeakyReLU()(y60)\n",
    "    y62 = MaxPooling2D(2,2,padding='same')(y61)\n",
    "    y63 = Conv2D(16*num_channels, (1,1), padding='same', use_bias=False)(y62)\n",
    "    y64 = BatchNormalization()(y63)\n",
    "    y65 = LeakyReLU()(y64)\n",
    "    y66 = Conv2D(16*num_channels, (3,3), padding='same', use_bias=False)(y65)\n",
    "    y67 = BatchNormalization()(y66)\n",
    "    y68 = LeakyReLU()(y67)\n",
    "    y69 = Conv2D(16*num_channels, (1,1), padding='same', use_bias=False)(y68)\n",
    "    y70 = BatchNormalization()(y69)\n",
    "    y71 = LeakyReLU()(y70)\n",
    "    y72 = Conv2D(16*num_channels, (3,3), padding='same', use_bias=False)(y71)\n",
    "    y73 = BatchNormalization()(y72)\n",
    "    y74 = LeakyReLU()(y73)\n",
    "    y75 = Concatenate()([y68, y74])\n",
    "    y76 = GlobalAveragePooling2D()(y75)\n",
    "    y77 = Flatten()(y76)\n",
    "    y78 = Dense(10)(y77)\n",
    "    y79 =  (y78)\n",
    "    return Model(inputs=y0, outputs=y79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "1j2INp6IxKjs",
    "outputId": "f2c1db48-fb55-49e0-f5d0-5c537dff8e9a"
   },
   "outputs": [],
   "source": [
    "m = build_model_20190712_010024(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AfblL2wqxOVC",
    "outputId": "5f6fb1e8-560b-4aea-c618-611f497a8d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_3 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_50 (Conv2D)              (None, 32, 32, 8)    24          input_3[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_50 (BatchNo (None, 32, 32, 8)    32          conv2d_50[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_40 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_50[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_51 (Conv2D)              (None, 32, 32, 8)    576         leaky_re_lu_40[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_51 (BatchNo (None, 32, 32, 8)    32          conv2d_51[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_41 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_51[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_52 (Conv2D)              (None, 32, 32, 8)    576         leaky_re_lu_41[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_52 (BatchNo (None, 32, 32, 8)    32          conv2d_52[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_42 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_52[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_53 (Conv2D)              (None, 32, 32, 8)    576         leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_53 (BatchNo (None, 32, 32, 8)    32          conv2d_53[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_54 (Conv2D)              (None, 32, 32, 8)    576         batch_normalization_53[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_57 (Conv2D)              (None, 32, 32, 8)    576         leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_60 (Conv2D)              (None, 32, 32, 8)    576         leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_54 (BatchNo (None, 32, 32, 8)    32          conv2d_54[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_57 (BatchNo (None, 32, 32, 8)    32          conv2d_57[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_60 (BatchNo (None, 32, 32, 8)    32          conv2d_60[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_63 (Conv2D)              (None, 32, 32, 8)    576         leaky_re_lu_42[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_43 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_54[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_58 (Conv2D)              (None, 32, 32, 8)    576         batch_normalization_57[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_47 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_60[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_63 (BatchNo (None, 32, 32, 8)    32          conv2d_63[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_55 (Conv2D)              (None, 32, 32, 8)    576         leaky_re_lu_43[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_58 (BatchNo (None, 32, 32, 8)    32          conv2d_58[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_61 (Conv2D)              (None, 32, 32, 8)    64          leaky_re_lu_47[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_50 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_63[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_55 (BatchNo (None, 32, 32, 8)    32          conv2d_55[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_45 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_58[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_61 (BatchNo (None, 32, 32, 8)    32          conv2d_61[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_64 (Conv2D)              (None, 32, 32, 8)    576         leaky_re_lu_50[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_56 (Conv2D)              (None, 32, 32, 8)    576         batch_normalization_55[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_59 (Conv2D)              (None, 32, 32, 8)    576         leaky_re_lu_45[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_48 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_61[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_64 (BatchNo (None, 32, 32, 8)    32          conv2d_64[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_56 (BatchNo (None, 32, 32, 8)    32          conv2d_56[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_59 (BatchNo (None, 32, 32, 8)    32          conv2d_59[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_62 (Conv2D)              (None, 32, 32, 8)    576         leaky_re_lu_48[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_65 (Conv2D)              (None, 32, 32, 8)    576         batch_normalization_64[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_44 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_56[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_46 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_59[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_62 (BatchNo (None, 32, 32, 8)    32          conv2d_62[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_65 (BatchNo (None, 32, 32, 8)    32          conv2d_65[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 32, 32, 8)    0           leaky_re_lu_43[0][0]             \n",
      "                                                                 leaky_re_lu_44[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 32, 32, 8)    0           leaky_re_lu_45[0][0]             \n",
      "                                                                 leaky_re_lu_46[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_49 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_62[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_51 (LeakyReLU)      (None, 32, 32, 8)    0           batch_normalization_65[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_4 (Concatenate)     (None, 32, 32, 32)   0           add_4[0][0]                      \n",
      "                                                                 add_5[0][0]                      \n",
      "                                                                 leaky_re_lu_49[0][0]             \n",
      "                                                                 leaky_re_lu_51[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_66 (Conv2D)              (None, 16, 16, 64)   18432       concatenate_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_66 (BatchNo (None, 16, 16, 64)   256         conv2d_66[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_52 (LeakyReLU)      (None, 16, 16, 64)   0           batch_normalization_66[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_67 (Conv2D)              (None, 16, 16, 64)   4096        leaky_re_lu_52[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_67 (BatchNo (None, 16, 16, 64)   256         conv2d_67[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_53 (LeakyReLU)      (None, 16, 16, 64)   0           batch_normalization_67[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_68 (Conv2D)              (None, 16, 16, 64)   36864       leaky_re_lu_53[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_68 (BatchNo (None, 16, 16, 64)   256         conv2d_68[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_54 (LeakyReLU)      (None, 16, 16, 64)   0           batch_normalization_68[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_69 (Conv2D)              (None, 16, 16, 64)   36864       leaky_re_lu_54[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_69 (BatchNo (None, 16, 16, 64)   256         conv2d_69[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_70 (Conv2D)              (None, 16, 16, 64)   36864       batch_normalization_69[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_70 (BatchNo (None, 16, 16, 64)   256         conv2d_70[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_55 (LeakyReLU)      (None, 16, 16, 64)   0           batch_normalization_70[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2D)  (None, 8, 8, 64)     0           leaky_re_lu_55[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_71 (Conv2D)              (None, 8, 8, 128)    8192        max_pooling2d_2[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_71 (BatchNo (None, 8, 8, 128)    512         conv2d_71[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_56 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_71[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_72 (Conv2D)              (None, 8, 8, 128)    147456      leaky_re_lu_56[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_72 (BatchNo (None, 8, 8, 128)    512         conv2d_72[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_57 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_72[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_73 (Conv2D)              (None, 8, 8, 128)    16384       leaky_re_lu_57[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_73 (BatchNo (None, 8, 8, 128)    512         conv2d_73[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_58 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_73[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_74 (Conv2D)              (None, 8, 8, 128)    147456      leaky_re_lu_58[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_74 (BatchNo (None, 8, 8, 128)    512         conv2d_74[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_59 (LeakyReLU)      (None, 8, 8, 128)    0           batch_normalization_74[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_5 (Concatenate)     (None, 8, 8, 256)    0           leaky_re_lu_57[0][0]             \n",
      "                                                                 leaky_re_lu_59[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d_2 (Glo (None, 256)          0           concatenate_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten_2 (Flatten)             (None, 256)          0           global_average_pooling2d_2[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 10)           2570        flatten_2[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 467,170\n",
      "Trainable params: 465,250\n",
      "Non-trainable params: 1,920\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0vIj7ud_M_c3",
    "outputId": "020aca85-b3d0-45fc-e9e3-dd79516979e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(m, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yX4PJJg0cXk"
   },
   "outputs": [],
   "source": [
    "def schedule_fn(epoch):\n",
    "  if epoch < 40:\n",
    "    return 1e-5 + 0.05 * epoch / 40.\n",
    "  elif epoch < 80:\n",
    "    return 0.05 - 0.05 * (epoch-40.) / 40.\n",
    "  else:\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_fn2(epoch):\n",
    "  if epoch < 80:\n",
    "    return 0.1\n",
    "  elif epoch < 120:\n",
    "    return 0.01\n",
    "  else:\n",
    "    return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQS1Mgwh0Yyu"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "lr_schedule_cb = keras.callbacks.LearningRateScheduler(schedule_fn2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_log_file = 'best2.csv'\n",
    "csv_logger_cb = keras.callbacks.CSVLogger(csv_log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C92W09IvtBpb"
   },
   "outputs": [],
   "source": [
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.SGD(lr=0.1, decay=1e-4, momentum=0.9)\n",
    "loss = lambda y_true, y_pred: tf.losses.softmax_cross_entropy(y_true, y_pred)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "m.compile(loss=loss,\n",
    "              optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "p1NaX7RctMQy",
    "outputId": "d36b8091-44b3-4550-8f85-9e3e38f1b298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "390/390 [==============================] - 21s 55ms/step - loss: 1.6339 - acc: 0.4003 - val_loss: 1.9608 - val_acc: 0.4143\n",
      "Epoch 2/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 1.2484 - acc: 0.5519 - val_loss: 1.5231 - val_acc: 0.4981\n",
      "Epoch 3/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 1.0619 - acc: 0.6215 - val_loss: 1.0264 - val_acc: 0.6407\n",
      "Epoch 4/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.9332 - acc: 0.6691 - val_loss: 1.0567 - val_acc: 0.6566\n",
      "Epoch 5/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.8362 - acc: 0.7070 - val_loss: 0.9179 - val_acc: 0.6857\n",
      "Epoch 6/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.7696 - acc: 0.7337 - val_loss: 0.8155 - val_acc: 0.7234\n",
      "Epoch 7/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.7043 - acc: 0.7563 - val_loss: 0.7117 - val_acc: 0.7585\n",
      "Epoch 8/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.6556 - acc: 0.7729 - val_loss: 0.8102 - val_acc: 0.7335\n",
      "Epoch 9/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.6176 - acc: 0.7881 - val_loss: 0.9724 - val_acc: 0.7080\n",
      "Epoch 10/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.5887 - acc: 0.7955 - val_loss: 0.6228 - val_acc: 0.7877\n",
      "Epoch 11/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.5652 - acc: 0.8066 - val_loss: 0.7237 - val_acc: 0.7567\n",
      "Epoch 12/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.5380 - acc: 0.8126 - val_loss: 0.6353 - val_acc: 0.7892\n",
      "Epoch 13/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.5150 - acc: 0.8214 - val_loss: 0.6394 - val_acc: 0.7880\n",
      "Epoch 14/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.4966 - acc: 0.8286 - val_loss: 0.5762 - val_acc: 0.8134\n",
      "Epoch 15/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.4810 - acc: 0.8332 - val_loss: 0.7466 - val_acc: 0.7633\n",
      "Epoch 16/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.4622 - acc: 0.8411 - val_loss: 0.6086 - val_acc: 0.7969\n",
      "Epoch 17/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.4483 - acc: 0.8448 - val_loss: 0.5648 - val_acc: 0.8161\n",
      "Epoch 18/160\n",
      "390/390 [==============================] - 17s 42ms/step - loss: 0.4347 - acc: 0.8503 - val_loss: 0.5398 - val_acc: 0.8281\n",
      "Epoch 19/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.4289 - acc: 0.8508 - val_loss: 0.5030 - val_acc: 0.8335\n",
      "Epoch 20/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.4131 - acc: 0.8555 - val_loss: 0.5544 - val_acc: 0.8166\n",
      "Epoch 21/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.4017 - acc: 0.8628 - val_loss: 0.5308 - val_acc: 0.8232\n",
      "Epoch 22/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.3917 - acc: 0.8648 - val_loss: 0.5780 - val_acc: 0.8139\n",
      "Epoch 23/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.3839 - acc: 0.8674 - val_loss: 0.4874 - val_acc: 0.8416\n",
      "Epoch 24/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.3746 - acc: 0.8701 - val_loss: 0.5146 - val_acc: 0.8304\n",
      "Epoch 25/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.3629 - acc: 0.8733 - val_loss: 0.4919 - val_acc: 0.8396\n",
      "Epoch 26/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.3628 - acc: 0.8743 - val_loss: 0.4532 - val_acc: 0.8535\n",
      "Epoch 27/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.3524 - acc: 0.8770 - val_loss: 0.4855 - val_acc: 0.8434\n",
      "Epoch 28/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.3438 - acc: 0.8800 - val_loss: 0.4677 - val_acc: 0.8476\n",
      "Epoch 29/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.3310 - acc: 0.8849 - val_loss: 0.4624 - val_acc: 0.8504\n",
      "Epoch 30/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.3278 - acc: 0.8849 - val_loss: 0.4896 - val_acc: 0.8411\n",
      "Epoch 31/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.3195 - acc: 0.8890 - val_loss: 0.5221 - val_acc: 0.8384\n",
      "Epoch 32/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.3147 - acc: 0.8906 - val_loss: 0.4594 - val_acc: 0.8596\n",
      "Epoch 33/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.3117 - acc: 0.8919 - val_loss: 0.4753 - val_acc: 0.8499\n",
      "Epoch 34/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.3046 - acc: 0.8940 - val_loss: 0.4395 - val_acc: 0.8608\n",
      "Epoch 35/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2967 - acc: 0.8961 - val_loss: 0.5018 - val_acc: 0.8432\n",
      "Epoch 36/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2953 - acc: 0.8955 - val_loss: 0.4709 - val_acc: 0.8530\n",
      "Epoch 37/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2847 - acc: 0.9007 - val_loss: 0.4800 - val_acc: 0.8476\n",
      "Epoch 38/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2815 - acc: 0.9016 - val_loss: 0.4665 - val_acc: 0.8574\n",
      "Epoch 39/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2771 - acc: 0.9045 - val_loss: 0.4713 - val_acc: 0.8563\n",
      "Epoch 40/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2717 - acc: 0.9042 - val_loss: 0.5400 - val_acc: 0.8363\n",
      "Epoch 41/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2708 - acc: 0.9058 - val_loss: 0.4432 - val_acc: 0.8633\n",
      "Epoch 42/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2658 - acc: 0.9071 - val_loss: 0.4513 - val_acc: 0.8645\n",
      "Epoch 43/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.2593 - acc: 0.9102 - val_loss: 0.4452 - val_acc: 0.8631\n",
      "Epoch 44/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2533 - acc: 0.9115 - val_loss: 0.4577 - val_acc: 0.8591\n",
      "Epoch 45/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2476 - acc: 0.9139 - val_loss: 0.4370 - val_acc: 0.8645\n",
      "Epoch 46/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2459 - acc: 0.9143 - val_loss: 0.4470 - val_acc: 0.8582\n",
      "Epoch 47/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2466 - acc: 0.9129 - val_loss: 0.3904 - val_acc: 0.8736\n",
      "Epoch 48/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2343 - acc: 0.9182 - val_loss: 0.4581 - val_acc: 0.8611\n",
      "Epoch 49/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2342 - acc: 0.9167 - val_loss: 0.4329 - val_acc: 0.8707\n",
      "Epoch 50/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2301 - acc: 0.9202 - val_loss: 0.4455 - val_acc: 0.8693\n",
      "Epoch 51/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2297 - acc: 0.9202 - val_loss: 0.4502 - val_acc: 0.8686\n",
      "Epoch 52/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2242 - acc: 0.9213 - val_loss: 0.4408 - val_acc: 0.8692\n",
      "Epoch 53/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.2196 - acc: 0.9235 - val_loss: 0.4741 - val_acc: 0.8611\n",
      "Epoch 54/160\n",
      "390/390 [==============================] - 18s 45ms/step - loss: 0.2156 - acc: 0.9238 - val_loss: 0.4257 - val_acc: 0.8744\n",
      "Epoch 55/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.2175 - acc: 0.9230 - val_loss: 0.4147 - val_acc: 0.8759\n",
      "Epoch 56/160\n",
      "390/390 [==============================] - 17s 45ms/step - loss: 0.2157 - acc: 0.9241 - val_loss: 0.4250 - val_acc: 0.8702\n",
      "Epoch 57/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.2057 - acc: 0.9277 - val_loss: 0.4076 - val_acc: 0.8745\n",
      "Epoch 58/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.2009 - acc: 0.9296 - val_loss: 0.3967 - val_acc: 0.8789\n",
      "Epoch 59/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2022 - acc: 0.9284 - val_loss: 0.4412 - val_acc: 0.8671\n",
      "Epoch 60/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.2006 - acc: 0.9285 - val_loss: 0.4283 - val_acc: 0.8721\n",
      "Epoch 61/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1958 - acc: 0.9309 - val_loss: 0.4384 - val_acc: 0.8745\n",
      "Epoch 62/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1930 - acc: 0.9312 - val_loss: 0.4069 - val_acc: 0.8790\n",
      "Epoch 63/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.1928 - acc: 0.9318 - val_loss: 0.4515 - val_acc: 0.8705\n",
      "Epoch 64/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.1890 - acc: 0.9338 - val_loss: 0.4565 - val_acc: 0.8684\n",
      "Epoch 65/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1850 - acc: 0.9351 - val_loss: 0.4833 - val_acc: 0.8635\n",
      "Epoch 66/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1810 - acc: 0.9361 - val_loss: 0.4552 - val_acc: 0.8719\n",
      "Epoch 67/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1829 - acc: 0.9351 - val_loss: 0.4365 - val_acc: 0.8755\n",
      "Epoch 68/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1778 - acc: 0.9371 - val_loss: 0.4372 - val_acc: 0.8754\n",
      "Epoch 69/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1766 - acc: 0.9374 - val_loss: 0.4378 - val_acc: 0.8771\n",
      "Epoch 70/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1731 - acc: 0.9387 - val_loss: 0.4494 - val_acc: 0.8728\n",
      "Epoch 71/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.1747 - acc: 0.9390 - val_loss: 0.4355 - val_acc: 0.8761\n",
      "Epoch 72/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.1678 - acc: 0.9397 - val_loss: 0.4502 - val_acc: 0.8756\n",
      "Epoch 73/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1709 - acc: 0.9405 - val_loss: 0.4687 - val_acc: 0.8710\n",
      "Epoch 74/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1654 - acc: 0.9414 - val_loss: 0.4226 - val_acc: 0.8804\n",
      "Epoch 75/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1599 - acc: 0.9438 - val_loss: 0.4475 - val_acc: 0.8783\n",
      "Epoch 76/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1625 - acc: 0.9428 - val_loss: 0.4546 - val_acc: 0.8754\n",
      "Epoch 77/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1589 - acc: 0.9431 - val_loss: 0.4398 - val_acc: 0.8777\n",
      "Epoch 78/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1561 - acc: 0.9438 - val_loss: 0.4444 - val_acc: 0.8773\n",
      "Epoch 79/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1517 - acc: 0.9456 - val_loss: 0.4627 - val_acc: 0.8742\n",
      "Epoch 80/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1543 - acc: 0.9447 - val_loss: 0.4428 - val_acc: 0.8768\n",
      "Epoch 81/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1325 - acc: 0.9529 - val_loss: 0.3719 - val_acc: 0.8914\n",
      "Epoch 82/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1127 - acc: 0.9618 - val_loss: 0.3654 - val_acc: 0.8927\n",
      "Epoch 83/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1105 - acc: 0.9625 - val_loss: 0.3658 - val_acc: 0.8951\n",
      "Epoch 84/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1085 - acc: 0.9620 - val_loss: 0.3687 - val_acc: 0.8924\n",
      "Epoch 85/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1067 - acc: 0.9631 - val_loss: 0.3699 - val_acc: 0.8929\n",
      "Epoch 86/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1063 - acc: 0.9631 - val_loss: 0.3696 - val_acc: 0.8955\n",
      "Epoch 87/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1036 - acc: 0.9645 - val_loss: 0.3736 - val_acc: 0.8936\n",
      "Epoch 88/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.1038 - acc: 0.9651 - val_loss: 0.3715 - val_acc: 0.8937\n",
      "Epoch 89/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.1011 - acc: 0.9647 - val_loss: 0.3726 - val_acc: 0.8940\n",
      "Epoch 90/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1001 - acc: 0.9662 - val_loss: 0.3695 - val_acc: 0.8936\n",
      "Epoch 91/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1023 - acc: 0.9654 - val_loss: 0.3714 - val_acc: 0.8952\n",
      "Epoch 92/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0999 - acc: 0.9659 - val_loss: 0.3723 - val_acc: 0.8943\n",
      "Epoch 93/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.1016 - acc: 0.9652 - val_loss: 0.3735 - val_acc: 0.8941\n",
      "Epoch 94/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0981 - acc: 0.9655 - val_loss: 0.3775 - val_acc: 0.8932\n",
      "Epoch 95/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0983 - acc: 0.9657 - val_loss: 0.3783 - val_acc: 0.8933\n",
      "Epoch 96/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0960 - acc: 0.9669 - val_loss: 0.3778 - val_acc: 0.8941\n",
      "Epoch 97/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0963 - acc: 0.9667 - val_loss: 0.3753 - val_acc: 0.8959\n",
      "Epoch 98/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0990 - acc: 0.9660 - val_loss: 0.3744 - val_acc: 0.8949\n",
      "Epoch 99/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0957 - acc: 0.9677 - val_loss: 0.3752 - val_acc: 0.8961\n",
      "Epoch 100/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0956 - acc: 0.9671 - val_loss: 0.3822 - val_acc: 0.8935\n",
      "Epoch 101/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0956 - acc: 0.9665 - val_loss: 0.3801 - val_acc: 0.8931\n",
      "Epoch 102/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0950 - acc: 0.9669 - val_loss: 0.3809 - val_acc: 0.8951\n",
      "Epoch 103/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0926 - acc: 0.9689 - val_loss: 0.3787 - val_acc: 0.8955\n",
      "Epoch 104/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0944 - acc: 0.9664 - val_loss: 0.3831 - val_acc: 0.8931\n",
      "Epoch 105/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0913 - acc: 0.9684 - val_loss: 0.3834 - val_acc: 0.8947\n",
      "Epoch 106/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0942 - acc: 0.9671 - val_loss: 0.3800 - val_acc: 0.8937\n",
      "Epoch 107/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0914 - acc: 0.9681 - val_loss: 0.3836 - val_acc: 0.8943\n",
      "Epoch 108/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0919 - acc: 0.9683 - val_loss: 0.3840 - val_acc: 0.8945\n",
      "Epoch 109/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0896 - acc: 0.9692 - val_loss: 0.3873 - val_acc: 0.8943\n",
      "Epoch 110/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0910 - acc: 0.9692 - val_loss: 0.3860 - val_acc: 0.8945\n",
      "Epoch 111/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0919 - acc: 0.9678 - val_loss: 0.3858 - val_acc: 0.8954\n",
      "Epoch 112/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0890 - acc: 0.9691 - val_loss: 0.3895 - val_acc: 0.8944\n",
      "Epoch 113/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0911 - acc: 0.9682 - val_loss: 0.3921 - val_acc: 0.8943\n",
      "Epoch 114/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0903 - acc: 0.9689 - val_loss: 0.3905 - val_acc: 0.8957\n",
      "Epoch 115/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0905 - acc: 0.9691 - val_loss: 0.3886 - val_acc: 0.8959\n",
      "Epoch 116/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0900 - acc: 0.9688 - val_loss: 0.3885 - val_acc: 0.8948\n",
      "Epoch 117/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0880 - acc: 0.9701 - val_loss: 0.3887 - val_acc: 0.8945\n",
      "Epoch 118/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0868 - acc: 0.9700 - val_loss: 0.3933 - val_acc: 0.8948\n",
      "Epoch 119/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0887 - acc: 0.9696 - val_loss: 0.3899 - val_acc: 0.8951\n",
      "Epoch 120/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0876 - acc: 0.9698 - val_loss: 0.3896 - val_acc: 0.8941\n",
      "Epoch 121/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0867 - acc: 0.9705 - val_loss: 0.3910 - val_acc: 0.8953\n",
      "Epoch 122/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0860 - acc: 0.9705 - val_loss: 0.3907 - val_acc: 0.8954\n",
      "Epoch 123/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0859 - acc: 0.9703 - val_loss: 0.3905 - val_acc: 0.8958\n",
      "Epoch 124/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0835 - acc: 0.9713 - val_loss: 0.3906 - val_acc: 0.8947\n",
      "Epoch 125/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0864 - acc: 0.9704 - val_loss: 0.3907 - val_acc: 0.8950\n",
      "Epoch 126/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0860 - acc: 0.9703 - val_loss: 0.3911 - val_acc: 0.8953\n",
      "Epoch 127/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0849 - acc: 0.9707 - val_loss: 0.3912 - val_acc: 0.8952\n",
      "Epoch 128/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0839 - acc: 0.9715 - val_loss: 0.3918 - val_acc: 0.8955\n",
      "Epoch 129/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0847 - acc: 0.9704 - val_loss: 0.3914 - val_acc: 0.8954\n",
      "Epoch 130/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0852 - acc: 0.9713 - val_loss: 0.3911 - val_acc: 0.8957\n",
      "Epoch 131/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0845 - acc: 0.9709 - val_loss: 0.3910 - val_acc: 0.8949\n",
      "Epoch 132/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0822 - acc: 0.9711 - val_loss: 0.3916 - val_acc: 0.8956\n",
      "Epoch 133/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0837 - acc: 0.9712 - val_loss: 0.3918 - val_acc: 0.8947\n",
      "Epoch 134/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0835 - acc: 0.9719 - val_loss: 0.3917 - val_acc: 0.8954\n",
      "Epoch 135/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0838 - acc: 0.9715 - val_loss: 0.3914 - val_acc: 0.8950\n",
      "Epoch 136/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0839 - acc: 0.9716 - val_loss: 0.3918 - val_acc: 0.8947\n",
      "Epoch 137/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0837 - acc: 0.9715 - val_loss: 0.3919 - val_acc: 0.8955\n",
      "Epoch 138/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0833 - acc: 0.9713 - val_loss: 0.3916 - val_acc: 0.8962\n",
      "Epoch 139/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0848 - acc: 0.9715 - val_loss: 0.3923 - val_acc: 0.8959\n",
      "Epoch 140/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0827 - acc: 0.9719 - val_loss: 0.3922 - val_acc: 0.8958\n",
      "Epoch 141/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0855 - acc: 0.9708 - val_loss: 0.3920 - val_acc: 0.8957\n",
      "Epoch 142/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0829 - acc: 0.9722 - val_loss: 0.3918 - val_acc: 0.8952\n",
      "Epoch 143/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0865 - acc: 0.9704 - val_loss: 0.3918 - val_acc: 0.8953\n",
      "Epoch 144/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0851 - acc: 0.9708 - val_loss: 0.3918 - val_acc: 0.8956\n",
      "Epoch 145/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0840 - acc: 0.9716 - val_loss: 0.3913 - val_acc: 0.8956\n",
      "Epoch 146/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0842 - acc: 0.9711 - val_loss: 0.3914 - val_acc: 0.8958\n",
      "Epoch 147/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0853 - acc: 0.9707 - val_loss: 0.3911 - val_acc: 0.8960\n",
      "Epoch 148/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0845 - acc: 0.9714 - val_loss: 0.3911 - val_acc: 0.8965\n",
      "Epoch 149/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0860 - acc: 0.9700 - val_loss: 0.3916 - val_acc: 0.8956\n",
      "Epoch 150/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0833 - acc: 0.9714 - val_loss: 0.3912 - val_acc: 0.8953\n",
      "Epoch 151/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0834 - acc: 0.9724 - val_loss: 0.3913 - val_acc: 0.8960\n",
      "Epoch 152/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0867 - acc: 0.9700 - val_loss: 0.3915 - val_acc: 0.8962\n",
      "Epoch 153/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0827 - acc: 0.9720 - val_loss: 0.3920 - val_acc: 0.8959\n",
      "Epoch 154/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0853 - acc: 0.9709 - val_loss: 0.3917 - val_acc: 0.8958\n",
      "Epoch 155/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0832 - acc: 0.9718 - val_loss: 0.3920 - val_acc: 0.8956\n",
      "Epoch 156/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0849 - acc: 0.9710 - val_loss: 0.3921 - val_acc: 0.8957\n",
      "Epoch 157/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0839 - acc: 0.9709 - val_loss: 0.3917 - val_acc: 0.8962\n",
      "Epoch 158/160\n",
      "390/390 [==============================] - 17s 44ms/step - loss: 0.0816 - acc: 0.9722 - val_loss: 0.3919 - val_acc: 0.8960\n",
      "Epoch 159/160\n",
      "390/390 [==============================] - 17s 45ms/step - loss: 0.0863 - acc: 0.9704 - val_loss: 0.3923 - val_acc: 0.8956\n",
      "Epoch 160/160\n",
      "390/390 [==============================] - 17s 43ms/step - loss: 0.0842 - acc: 0.9706 - val_loss: 0.3921 - val_acc: 0.8958\n"
     ]
    }
   ],
   "source": [
    "history = m.fit(train_dataset, steps_per_epoch=390, epochs=160, validation_data=test_dataset, callbacks=[lr_schedule_cb, csv_logger_cb], validation_steps=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yO6f7GWdY11E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GGGFC-CIFAR10-Trainer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
