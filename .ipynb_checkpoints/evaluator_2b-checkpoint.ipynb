{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "almqXdO8O-03"
   },
   "source": [
    "# Trainer for Graph Generated Computer Vision Models in Keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "RRsArs2dq-MM",
    "outputId": "516e8a80-4128-4231-dcac-b83e5fca304a"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Klx1YtLfPBHL"
   },
   "source": [
    "## Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "NfGn11dJq-pJ",
    "outputId": "297202c7-9fbc-4451-a75d-83cff429f883"
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((x_train[0:8] / 255).astype(np.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4TnNzMSJNnui"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.utils import to_categorical\n",
    "y_train = to_categorical(y_train, num_classes=10, dtype='float32')\n",
    "y_test = to_categorical(y_test, num_classes=10, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wwdHh5wiq-Ex"
   },
   "outputs": [],
   "source": [
    "train_dataset_ = tf.data.Dataset.from_tensor_slices(((x_train / 255.).astype(np.float32), y_train))\n",
    "test_dataset_ = tf.data.Dataset.from_tensor_slices(((x_test / 255.).astype(np.float32), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "uj07jjDiNDJB",
    "outputId": "90f9cea4-5495-4d07-9157-77fdfb2ef8a4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((32, 32, 3), (10,)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_8UYJcHnwoyO"
   },
   "outputs": [],
   "source": [
    "def augment(x,y) -> (tf.Tensor, tf.Tensor):\n",
    "  x = tf.image.random_flip_left_right(x)\n",
    "  x = tf.image.random_brightness(x, 0.05)\n",
    "  x = tf.image.random_contrast(x, 0.7,1.3)\n",
    "  x = tf.image.random_hue(x, 0.08)\n",
    "  x = tf.clip_by_value(x, 0., 1.)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def augment2(x,y) -> (tf.Tensor, tf.Tensor):\n",
    "#   x = tf.image.resize_images(x, (36,36), align_corners=True)\n",
    "  x = tf.image.per_image_standardization(x)\n",
    "  x = tf.image.random_flip_left_right(x)\n",
    "  x = tf.image.pad_to_bounding_box(x,4, 4, 40,40)\n",
    "  x = tf.image.random_crop(x, (32,32,3))\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize(x,y):\n",
    "    x = tf.image.per_image_standardization(x)\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iJRxzIzbq-zB"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0724 08:39:45.790807 10148 deprecation.py:323] From c:\\users\\holcm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\image_ops_impl.py:1514: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Deprecated in favor of operator or tf.math.divide.\n"
     ]
    }
   ],
   "source": [
    "train_dataset = train_dataset_.map(augment2,num_parallel_calls=4).shuffle(buffer_size=1000).batch(128).repeat()\n",
    "test_dataset = test_dataset_.map(standardize,num_parallel_calls=4).batch(128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "e_RY7qX3NFm7",
    "outputId": "2b58711a-d81d-40c8-a8de-d7a8bc383e7e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<DatasetV1Adapter shapes: ((?, 32, 32, 3), (?, 10)), types: (tf.float32, tf.float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdZ4krANPJGS"
   },
   "source": [
    "## Build Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "U9Rl_vZ_ldm1"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Input, Conv2D, Concatenate, BatchNormalization, LeakyReLU, Add, MaxPooling2D, Flatten, Dense, GlobalMaxPooling2D, GlobalAveragePooling2D, Softmax\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H6yJ0xRtxJkZ"
   },
   "outputs": [],
   "source": [
    "def build_model_20190712_010024(num_channels):\n",
    "    y0 = Input(shape=(32,32,3))\n",
    "    y1 = Conv2D(1*num_channels, (1,1), padding='same', use_bias=False)(y0)\n",
    "    y2 = BatchNormalization()(y1)\n",
    "    y3 = LeakyReLU()(y2)\n",
    "    y4 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y3)\n",
    "    y5 = BatchNormalization()(y4)\n",
    "    y6 = LeakyReLU()(y5)\n",
    "    y7 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y6)\n",
    "    y8 = BatchNormalization()(y7)\n",
    "    y9 = LeakyReLU()(y8)\n",
    "    y10 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y9)\n",
    "    y11 = BatchNormalization()(y10)\n",
    "    y12 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y11)\n",
    "    y13 = BatchNormalization()(y12)\n",
    "    y14 = LeakyReLU()(y13)\n",
    "    y15 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y14)\n",
    "    y16 = BatchNormalization()(y15)\n",
    "    y17 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y16)\n",
    "    y18 = BatchNormalization()(y17)\n",
    "    y19 = LeakyReLU()(y18)\n",
    "    y20 = Add()([y14, y19])\n",
    "    y21 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y9)\n",
    "    y22 = BatchNormalization()(y21)\n",
    "    y23 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y22)\n",
    "    y24 = BatchNormalization()(y23)\n",
    "    y25 = LeakyReLU()(y24)\n",
    "    y26 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y25)\n",
    "    y27 = BatchNormalization()(y26)\n",
    "    y28 = LeakyReLU()(y27)\n",
    "    y29 = Add()([y25, y28])\n",
    "    y30 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y9)\n",
    "    y31 = BatchNormalization()(y30)\n",
    "    y32 = LeakyReLU()(y31)\n",
    "    y33 = Conv2D(1*num_channels, (1,1), padding='same', use_bias=False)(y32)\n",
    "    y34 = BatchNormalization()(y33)\n",
    "    y35 = LeakyReLU()(y34)\n",
    "    y36 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y35)\n",
    "    y37 = BatchNormalization()(y36)\n",
    "    y38 = LeakyReLU()(y37)\n",
    "    y39 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y9)\n",
    "    y40 = BatchNormalization()(y39)\n",
    "    y41 = LeakyReLU()(y40)\n",
    "    y42 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y41)\n",
    "    y43 = BatchNormalization()(y42)\n",
    "    y44 = Conv2D(1*num_channels, (3,3), padding='same', use_bias=False)(y43)\n",
    "    y45 = BatchNormalization()(y44)\n",
    "    y46 = LeakyReLU()(y45)\n",
    "    y47 = Concatenate()([y20, y29, y38, y46])\n",
    "    y48 = Conv2D(8*num_channels, (3,3), (2,2), padding='same', use_bias=False)(y47)\n",
    "    y49 = BatchNormalization()(y48)\n",
    "    y50 = LeakyReLU()(y49)\n",
    "    y51 = Conv2D(8*num_channels, (1,1), padding='same', use_bias=False)(y50)\n",
    "    y52 = BatchNormalization()(y51)\n",
    "    y53 = LeakyReLU()(y52)\n",
    "    y54 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y53)\n",
    "    y55 = BatchNormalization()(y54)\n",
    "    y56 = LeakyReLU()(y55)\n",
    "    y57 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y56)\n",
    "    y58 = BatchNormalization()(y57)\n",
    "    y59 = Conv2D(8*num_channels, (3,3), padding='same', use_bias=False)(y58)\n",
    "    y60 = BatchNormalization()(y59)\n",
    "    y61 = LeakyReLU()(y60)\n",
    "    y62 = MaxPooling2D(2,2,padding='same')(y61)\n",
    "    y63 = Conv2D(16*num_channels, (1,1), padding='same', use_bias=False)(y62)\n",
    "    y64 = BatchNormalization()(y63)\n",
    "    y65 = LeakyReLU()(y64)\n",
    "    y66 = Conv2D(16*num_channels, (3,3), padding='same', use_bias=False)(y65)\n",
    "    y67 = BatchNormalization()(y66)\n",
    "    y68 = LeakyReLU()(y67)\n",
    "    y69 = Conv2D(16*num_channels, (1,1), padding='same', use_bias=False)(y68)\n",
    "    y70 = BatchNormalization()(y69)\n",
    "    y71 = LeakyReLU()(y70)\n",
    "    y72 = Conv2D(16*num_channels, (3,3), padding='same', use_bias=False)(y71)\n",
    "    y73 = BatchNormalization()(y72)\n",
    "    y74 = LeakyReLU()(y73)\n",
    "    y75 = Concatenate()([y68, y74])\n",
    "    y76 = GlobalAveragePooling2D()(y75)\n",
    "    y77 = Flatten()(y76)\n",
    "    y78 = Dense(10)(y77)\n",
    "    y79 =  (y78)\n",
    "    return Model(inputs=y0, outputs=y79)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "1j2INp6IxKjs",
    "outputId": "f2c1db48-fb55-49e0-f5d0-5c537dff8e9a"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 08:39:46.009224 10148 deprecation.py:506] From c:\\users\\holcm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    }
   ],
   "source": [
    "m = build_model_20190712_010024(16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "AfblL2wqxOVC",
    "outputId": "5f6fb1e8-560b-4aea-c618-611f497a8d88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, 32, 32, 3)]  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv2d (Conv2D)                 (None, 32, 32, 16)   48          input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization (BatchNorma (None, 32, 32, 16)   64          conv2d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)         (None, 32, 32, 16)   0           batch_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 32, 32, 16)   2304        leaky_re_lu[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNor (None, 32, 32, 16)   64          conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_1[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 32, 32, 16)   2304        leaky_re_lu_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNor (None, 32, 32, 16)   64          conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_2[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 32, 32, 16)   2304        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNor (None, 32, 32, 16)   64          conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 32, 32, 16)   2304        batch_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_7 (Conv2D)               (None, 32, 32, 16)   2304        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_10 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_4 (BatchNor (None, 32, 32, 16)   64          conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_7 (BatchNor (None, 32, 32, 16)   64          conv2d_7[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_10 (BatchNo (None, 32, 32, 16)   64          conv2d_10[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_13 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_2[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_8 (Conv2D)               (None, 32, 32, 16)   2304        batch_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_13 (BatchNo (None, 32, 32, 16)   64          conv2d_13[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_5 (Conv2D)               (None, 32, 32, 16)   2304        leaky_re_lu_3[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_8 (BatchNor (None, 32, 32, 16)   64          conv2d_8[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_11 (Conv2D)              (None, 32, 32, 16)   256         leaky_re_lu_7[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_10 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_13[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_5 (BatchNor (None, 32, 32, 16)   64          conv2d_5[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_11 (BatchNo (None, 32, 32, 16)   64          conv2d_11[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_14 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_10[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_6 (Conv2D)               (None, 32, 32, 16)   2304        batch_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_9 (Conv2D)               (None, 32, 32, 16)   2304        leaky_re_lu_5[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_8 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_11[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_14 (BatchNo (None, 32, 32, 16)   64          conv2d_14[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_6 (BatchNor (None, 32, 32, 16)   64          conv2d_6[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_9 (BatchNor (None, 32, 32, 16)   64          conv2d_9[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_12 (Conv2D)              (None, 32, 32, 16)   2304        leaky_re_lu_8[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_15 (Conv2D)              (None, 32, 32, 16)   2304        batch_normalization_14[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_12 (BatchNo (None, 32, 32, 16)   64          conv2d_12[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_15 (BatchNo (None, 32, 32, 16)   64          conv2d_15[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "add (Add)                       (None, 32, 32, 16)   0           leaky_re_lu_3[0][0]              \n",
      "                                                                 leaky_re_lu_4[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "add_1 (Add)                     (None, 32, 32, 16)   0           leaky_re_lu_5[0][0]              \n",
      "                                                                 leaky_re_lu_6[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_9 (LeakyReLU)       (None, 32, 32, 16)   0           batch_normalization_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_11 (LeakyReLU)      (None, 32, 32, 16)   0           batch_normalization_15[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate (Concatenate)       (None, 32, 32, 64)   0           add[0][0]                        \n",
      "                                                                 add_1[0][0]                      \n",
      "                                                                 leaky_re_lu_9[0][0]              \n",
      "                                                                 leaky_re_lu_11[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_16 (Conv2D)              (None, 16, 16, 128)  73728       concatenate[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_16 (BatchNo (None, 16, 16, 128)  512         conv2d_16[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_12 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_16[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_17 (Conv2D)              (None, 16, 16, 128)  16384       leaky_re_lu_12[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_17 (BatchNo (None, 16, 16, 128)  512         conv2d_17[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_13 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_17[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_18 (Conv2D)              (None, 16, 16, 128)  147456      leaky_re_lu_13[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_18 (BatchNo (None, 16, 16, 128)  512         conv2d_18[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_14 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_18[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_19 (Conv2D)              (None, 16, 16, 128)  147456      leaky_re_lu_14[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_19 (BatchNo (None, 16, 16, 128)  512         conv2d_19[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_20 (Conv2D)              (None, 16, 16, 128)  147456      batch_normalization_19[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_20 (BatchNo (None, 16, 16, 128)  512         conv2d_20[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_15 (LeakyReLU)      (None, 16, 16, 128)  0           batch_normalization_20[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D)    (None, 8, 8, 128)    0           leaky_re_lu_15[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_21 (Conv2D)              (None, 8, 8, 256)    32768       max_pooling2d[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_21 (BatchNo (None, 8, 8, 256)    1024        conv2d_21[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_16 (LeakyReLU)      (None, 8, 8, 256)    0           batch_normalization_21[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_22 (Conv2D)              (None, 8, 8, 256)    589824      leaky_re_lu_16[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_22 (BatchNo (None, 8, 8, 256)    1024        conv2d_22[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_17 (LeakyReLU)      (None, 8, 8, 256)    0           batch_normalization_22[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_23 (Conv2D)              (None, 8, 8, 256)    65536       leaky_re_lu_17[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_23 (BatchNo (None, 8, 8, 256)    1024        conv2d_23[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_18 (LeakyReLU)      (None, 8, 8, 256)    0           batch_normalization_23[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_24 (Conv2D)              (None, 8, 8, 256)    589824      leaky_re_lu_18[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "batch_normalization_24 (BatchNo (None, 8, 8, 256)    1024        conv2d_24[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "leaky_re_lu_19 (LeakyReLU)      (None, 8, 8, 256)    0           batch_normalization_24[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 8, 8, 512)    0           leaky_re_lu_17[0][0]             \n",
      "                                                                 leaky_re_lu_19[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling2d (Globa (None, 512)          0           concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "flatten (Flatten)               (None, 512)          0           global_average_pooling2d[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 10)           5130        flatten[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,855,802\n",
      "Trainable params: 1,851,962\n",
      "Non-trainable params: 3,840\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "m.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "0vIj7ud_M_c3",
    "outputId": "020aca85-b3d0-45fc-e9e3-dd79516979e3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to import pydot. You must install pydot and graphviz for `pydotprint` to work.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.utils import plot_model\n",
    "plot_model(m, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3yX4PJJg0cXk"
   },
   "outputs": [],
   "source": [
    "def schedule_fn(epoch):\n",
    "  if epoch < 40:\n",
    "    return 1e-5 + 0.05 * epoch / 40.\n",
    "  elif epoch < 80:\n",
    "    return 0.05 - 0.05 * (epoch-40.) / 40.\n",
    "  else:\n",
    "    return 1e-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def schedule_fn2(epoch):\n",
    "  if epoch < 80:\n",
    "    return 0.1\n",
    "  elif epoch < 120:\n",
    "    return 0.01\n",
    "  else:\n",
    "    return 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nQS1Mgwh0Yyu"
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "lr_schedule_cb = keras.callbacks.LearningRateScheduler(schedule_fn2, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_log_file = 'best2b.csv'\n",
    "csv_logger_cb = keras.callbacks.CSVLogger(csv_log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C92W09IvtBpb"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0724 08:40:10.288088 10148 deprecation.py:323] From c:\\users\\holcm\\miniconda3\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# initiate RMSprop optimizer\n",
    "opt = keras.optimizers.SGD(lr=0.1, decay=1e-4, momentum=0.9)\n",
    "loss = lambda y_true, y_pred: tf.losses.softmax_cross_entropy(y_true, y_pred)\n",
    "\n",
    "# Let's train the model using RMSprop\n",
    "m.compile(loss=loss,\n",
    "              optimizer=opt, metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "p1NaX7RctMQy",
    "outputId": "d36b8091-44b3-4550-8f85-9e3e38f1b298"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/160\n",
      "390/390 [==============================] - 32s 82ms/step - loss: 1.7115 - acc: 0.3740 - val_loss: 1.9991 - val_acc: 0.3941\n",
      "Epoch 2/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 1.2397 - acc: 0.5564 - val_loss: 1.3870 - val_acc: 0.5349\n",
      "Epoch 3/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 1.0179 - acc: 0.6386 - val_loss: 1.0810 - val_acc: 0.6318\n",
      "Epoch 4/160\n",
      "390/390 [==============================] - 26s 65ms/step - loss: 0.8825 - acc: 0.6897 - val_loss: 1.2582 - val_acc: 0.6016\n",
      "Epoch 5/160\n",
      "390/390 [==============================] - 25s 65ms/step - loss: 0.7912 - acc: 0.7242 - val_loss: 0.9339 - val_acc: 0.6998\n",
      "Epoch 6/160\n",
      "390/390 [==============================] - 25s 65ms/step - loss: 0.7162 - acc: 0.7518 - val_loss: 0.7856 - val_acc: 0.7354\n",
      "Epoch 7/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.6586 - acc: 0.7703 - val_loss: 0.7379 - val_acc: 0.7555\n",
      "Epoch 8/160\n",
      "390/390 [==============================] - 26s 65ms/step - loss: 0.6162 - acc: 0.7858 - val_loss: 0.8192 - val_acc: 0.7363\n",
      "Epoch 9/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.5744 - acc: 0.8011 - val_loss: 0.7515 - val_acc: 0.7601\n",
      "Epoch 10/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.5401 - acc: 0.8144 - val_loss: 0.8606 - val_acc: 0.7251\n",
      "Epoch 11/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.5131 - acc: 0.8218 - val_loss: 0.6986 - val_acc: 0.7757\n",
      "Epoch 12/160\n",
      "390/390 [==============================] - 26s 65ms/step - loss: 0.4894 - acc: 0.8301 - val_loss: 0.5517 - val_acc: 0.8068\n",
      "Epoch 13/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.4663 - acc: 0.8402 - val_loss: 0.5460 - val_acc: 0.8148\n",
      "Epoch 14/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.4458 - acc: 0.8456 - val_loss: 0.6024 - val_acc: 0.8057\n",
      "Epoch 15/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.4303 - acc: 0.8516 - val_loss: 0.5237 - val_acc: 0.8262\n",
      "Epoch 16/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.4122 - acc: 0.8578 - val_loss: 0.6277 - val_acc: 0.7964\n",
      "Epoch 17/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.3953 - acc: 0.8636 - val_loss: 0.4974 - val_acc: 0.8353\n",
      "Epoch 18/160\n",
      "390/390 [==============================] - 26s 65ms/step - loss: 0.3806 - acc: 0.8673 - val_loss: 0.6018 - val_acc: 0.8137\n",
      "Epoch 19/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.3686 - acc: 0.8734 - val_loss: 0.5826 - val_acc: 0.8121\n",
      "Epoch 20/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.3491 - acc: 0.8782 - val_loss: 0.5635 - val_acc: 0.8226\n",
      "Epoch 21/160\n",
      "390/390 [==============================] - 27s 68ms/step - loss: 0.3420 - acc: 0.8815 - val_loss: 0.5460 - val_acc: 0.8248\n",
      "Epoch 22/160\n",
      "390/390 [==============================] - 27s 69ms/step - loss: 0.3303 - acc: 0.8847 - val_loss: 0.5742 - val_acc: 0.8213\n",
      "Epoch 23/160\n",
      "390/390 [==============================] - 26s 68ms/step - loss: 0.3222 - acc: 0.8870 - val_loss: 0.4730 - val_acc: 0.8398\n",
      "Epoch 24/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.3035 - acc: 0.8950 - val_loss: 0.4796 - val_acc: 0.8485\n",
      "Epoch 25/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.3010 - acc: 0.8953 - val_loss: 0.5239 - val_acc: 0.8335\n",
      "Epoch 26/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.2879 - acc: 0.9008 - val_loss: 0.4392 - val_acc: 0.8564\n",
      "Epoch 27/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.2833 - acc: 0.9017 - val_loss: 0.4248 - val_acc: 0.8620\n",
      "Epoch 28/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.2770 - acc: 0.9025 - val_loss: 0.4849 - val_acc: 0.8374\n",
      "Epoch 29/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.2684 - acc: 0.9071 - val_loss: 0.4088 - val_acc: 0.8657\n",
      "Epoch 30/160\n",
      "390/390 [==============================] - 26s 68ms/step - loss: 0.2580 - acc: 0.9101 - val_loss: 0.4172 - val_acc: 0.8634\n",
      "Epoch 31/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.2502 - acc: 0.9119 - val_loss: 0.4364 - val_acc: 0.8605\n",
      "Epoch 32/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.2428 - acc: 0.9143 - val_loss: 0.5734 - val_acc: 0.8282\n",
      "Epoch 33/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.2344 - acc: 0.9170 - val_loss: 0.4868 - val_acc: 0.8640\n",
      "Epoch 34/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.2314 - acc: 0.9188 - val_loss: 0.4226 - val_acc: 0.8720\n",
      "Epoch 35/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.2248 - acc: 0.9210 - val_loss: 0.4857 - val_acc: 0.8545\n",
      "Epoch 36/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.2195 - acc: 0.9226 - val_loss: 0.4505 - val_acc: 0.8627\n",
      "Epoch 37/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.2155 - acc: 0.9241 - val_loss: 0.4401 - val_acc: 0.8639\n",
      "Epoch 38/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.2074 - acc: 0.9274 - val_loss: 0.4364 - val_acc: 0.8668\n",
      "Epoch 39/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1999 - acc: 0.9299 - val_loss: 0.4225 - val_acc: 0.8697\n",
      "Epoch 40/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1971 - acc: 0.9299 - val_loss: 0.4160 - val_acc: 0.8763\n",
      "Epoch 41/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.1910 - acc: 0.9324 - val_loss: 0.4700 - val_acc: 0.8642\n",
      "Epoch 42/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1907 - acc: 0.9326 - val_loss: 0.4338 - val_acc: 0.8720\n",
      "Epoch 43/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.1774 - acc: 0.9367 - val_loss: 0.4244 - val_acc: 0.8731\n",
      "Epoch 44/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.1787 - acc: 0.9366 - val_loss: 0.4265 - val_acc: 0.8758\n",
      "Epoch 45/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.1711 - acc: 0.9397 - val_loss: 0.4157 - val_acc: 0.8806\n",
      "Epoch 46/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1680 - acc: 0.9398 - val_loss: 0.4078 - val_acc: 0.8816\n",
      "Epoch 47/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.1643 - acc: 0.9424 - val_loss: 0.4015 - val_acc: 0.8864\n",
      "Epoch 48/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.1633 - acc: 0.9421 - val_loss: 0.4511 - val_acc: 0.8742\n",
      "Epoch 49/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1549 - acc: 0.9454 - val_loss: 0.4242 - val_acc: 0.8802\n",
      "Epoch 50/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.1536 - acc: 0.9462 - val_loss: 0.4369 - val_acc: 0.8785\n",
      "Epoch 51/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.1451 - acc: 0.9481 - val_loss: 0.4997 - val_acc: 0.8698\n",
      "Epoch 52/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.1454 - acc: 0.9485 - val_loss: 0.4392 - val_acc: 0.8769\n",
      "Epoch 53/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.1399 - acc: 0.9501 - val_loss: 0.4523 - val_acc: 0.8813\n",
      "Epoch 54/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1409 - acc: 0.9488 - val_loss: 0.4667 - val_acc: 0.8835\n",
      "Epoch 55/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1321 - acc: 0.9529 - val_loss: 0.4286 - val_acc: 0.8835\n",
      "Epoch 56/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1324 - acc: 0.9531 - val_loss: 0.4203 - val_acc: 0.8873\n",
      "Epoch 57/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1281 - acc: 0.9541 - val_loss: 0.4301 - val_acc: 0.8852\n",
      "Epoch 58/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1234 - acc: 0.9568 - val_loss: 0.4275 - val_acc: 0.8800\n",
      "Epoch 59/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1213 - acc: 0.9570 - val_loss: 0.4224 - val_acc: 0.8913\n",
      "Epoch 60/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1190 - acc: 0.9569 - val_loss: 0.4419 - val_acc: 0.8848\n",
      "Epoch 61/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1156 - acc: 0.9588 - val_loss: 0.4551 - val_acc: 0.8842\n",
      "Epoch 62/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1159 - acc: 0.9586 - val_loss: 0.4539 - val_acc: 0.8823\n",
      "Epoch 63/160\n",
      "390/390 [==============================] - 26s 68ms/step - loss: 0.1090 - acc: 0.9602 - val_loss: 0.4806 - val_acc: 0.8787\n",
      "Epoch 64/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.1059 - acc: 0.9623 - val_loss: 0.4568 - val_acc: 0.8829\n",
      "Epoch 65/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1005 - acc: 0.9637 - val_loss: 0.4532 - val_acc: 0.8835\n",
      "Epoch 66/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1053 - acc: 0.9623 - val_loss: 0.5193 - val_acc: 0.8762\n",
      "Epoch 67/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1012 - acc: 0.9645 - val_loss: 0.4916 - val_acc: 0.8858\n",
      "Epoch 68/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.1008 - acc: 0.9641 - val_loss: 0.4410 - val_acc: 0.8886\n",
      "Epoch 69/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0950 - acc: 0.9676 - val_loss: 0.4898 - val_acc: 0.8846\n",
      "Epoch 70/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0926 - acc: 0.9673 - val_loss: 0.4685 - val_acc: 0.8887\n",
      "Epoch 71/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0870 - acc: 0.9683 - val_loss: 0.4179 - val_acc: 0.8947\n",
      "Epoch 72/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0882 - acc: 0.9681 - val_loss: 0.4648 - val_acc: 0.8925\n",
      "Epoch 73/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0873 - acc: 0.9681 - val_loss: 0.4574 - val_acc: 0.8865\n",
      "Epoch 74/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0863 - acc: 0.9690 - val_loss: 0.5192 - val_acc: 0.8810\n",
      "Epoch 75/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0853 - acc: 0.9689 - val_loss: 0.4604 - val_acc: 0.8957\n",
      "Epoch 76/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0854 - acc: 0.9692 - val_loss: 0.5124 - val_acc: 0.8844\n",
      "Epoch 77/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0798 - acc: 0.9712 - val_loss: 0.4933 - val_acc: 0.8830\n",
      "Epoch 78/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0730 - acc: 0.9741 - val_loss: 0.4747 - val_acc: 0.8886\n",
      "Epoch 79/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0746 - acc: 0.9727 - val_loss: 0.5353 - val_acc: 0.8799\n",
      "Epoch 80/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0749 - acc: 0.9725 - val_loss: 0.4767 - val_acc: 0.8877\n",
      "Epoch 81/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0563 - acc: 0.9799 - val_loss: 0.3935 - val_acc: 0.9089\n",
      "Epoch 82/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0433 - acc: 0.9856 - val_loss: 0.3929 - val_acc: 0.9090\n",
      "Epoch 83/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0391 - acc: 0.9875 - val_loss: 0.3934 - val_acc: 0.9096\n",
      "Epoch 84/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0393 - acc: 0.9868 - val_loss: 0.3915 - val_acc: 0.9104\n",
      "Epoch 85/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0360 - acc: 0.9888 - val_loss: 0.3903 - val_acc: 0.9087\n",
      "Epoch 86/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0361 - acc: 0.9880 - val_loss: 0.3914 - val_acc: 0.9102\n",
      "Epoch 87/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0362 - acc: 0.9882 - val_loss: 0.3947 - val_acc: 0.9100\n",
      "Epoch 88/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0341 - acc: 0.9894 - val_loss: 0.3981 - val_acc: 0.9101\n",
      "Epoch 89/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0347 - acc: 0.9894 - val_loss: 0.3927 - val_acc: 0.9099\n",
      "Epoch 90/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0337 - acc: 0.9897 - val_loss: 0.3946 - val_acc: 0.9095\n",
      "Epoch 91/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0318 - acc: 0.9903 - val_loss: 0.3956 - val_acc: 0.9094\n",
      "Epoch 92/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0315 - acc: 0.9904 - val_loss: 0.4001 - val_acc: 0.9089\n",
      "Epoch 93/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0302 - acc: 0.9913 - val_loss: 0.3968 - val_acc: 0.9086\n",
      "Epoch 94/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0318 - acc: 0.9900 - val_loss: 0.4004 - val_acc: 0.9099\n",
      "Epoch 95/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0314 - acc: 0.9902 - val_loss: 0.4002 - val_acc: 0.9095\n",
      "Epoch 96/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0284 - acc: 0.9915 - val_loss: 0.4029 - val_acc: 0.9091\n",
      "Epoch 97/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0299 - acc: 0.9909 - val_loss: 0.4014 - val_acc: 0.9096\n",
      "Epoch 98/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0291 - acc: 0.9912 - val_loss: 0.4007 - val_acc: 0.9096\n",
      "Epoch 99/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0292 - acc: 0.9912 - val_loss: 0.4055 - val_acc: 0.9107\n",
      "Epoch 100/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0289 - acc: 0.9912 - val_loss: 0.4052 - val_acc: 0.9104\n",
      "Epoch 101/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0274 - acc: 0.9923 - val_loss: 0.4064 - val_acc: 0.9093\n",
      "Epoch 102/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0280 - acc: 0.9917 - val_loss: 0.4058 - val_acc: 0.9100\n",
      "Epoch 103/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0278 - acc: 0.9913 - val_loss: 0.4052 - val_acc: 0.9094\n",
      "Epoch 104/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0270 - acc: 0.9921 - val_loss: 0.4080 - val_acc: 0.9090\n",
      "Epoch 105/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0275 - acc: 0.9913 - val_loss: 0.4096 - val_acc: 0.9081\n",
      "Epoch 106/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0275 - acc: 0.9917 - val_loss: 0.4082 - val_acc: 0.9097\n",
      "Epoch 107/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0260 - acc: 0.9925 - val_loss: 0.4104 - val_acc: 0.9090\n",
      "Epoch 108/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0266 - acc: 0.9924 - val_loss: 0.4078 - val_acc: 0.9087\n",
      "Epoch 109/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0260 - acc: 0.9923 - val_loss: 0.4116 - val_acc: 0.9098\n",
      "Epoch 110/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0279 - acc: 0.9913 - val_loss: 0.4169 - val_acc: 0.9091\n",
      "Epoch 111/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0260 - acc: 0.9924 - val_loss: 0.4143 - val_acc: 0.9093\n",
      "Epoch 112/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0264 - acc: 0.9921 - val_loss: 0.4141 - val_acc: 0.9084\n",
      "Epoch 113/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0253 - acc: 0.9923 - val_loss: 0.4149 - val_acc: 0.9088\n",
      "Epoch 114/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0255 - acc: 0.9927 - val_loss: 0.4141 - val_acc: 0.9083\n",
      "Epoch 115/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0257 - acc: 0.9921 - val_loss: 0.4177 - val_acc: 0.9091\n",
      "Epoch 116/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0246 - acc: 0.9928 - val_loss: 0.4209 - val_acc: 0.9091\n",
      "Epoch 117/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0238 - acc: 0.9928 - val_loss: 0.4196 - val_acc: 0.9091\n",
      "Epoch 118/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0255 - acc: 0.9922 - val_loss: 0.4203 - val_acc: 0.9088\n",
      "Epoch 119/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0241 - acc: 0.9927 - val_loss: 0.4195 - val_acc: 0.9082\n",
      "Epoch 120/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0254 - acc: 0.9927 - val_loss: 0.4196 - val_acc: 0.9095\n",
      "Epoch 121/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0246 - acc: 0.9926 - val_loss: 0.4192 - val_acc: 0.9087\n",
      "Epoch 122/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0235 - acc: 0.9934 - val_loss: 0.4185 - val_acc: 0.9085\n",
      "Epoch 123/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0236 - acc: 0.9933 - val_loss: 0.4181 - val_acc: 0.9090\n",
      "Epoch 124/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0233 - acc: 0.9934 - val_loss: 0.4180 - val_acc: 0.9093\n",
      "Epoch 125/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0237 - acc: 0.9931 - val_loss: 0.4176 - val_acc: 0.9091\n",
      "Epoch 126/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0234 - acc: 0.9935 - val_loss: 0.4175 - val_acc: 0.9090\n",
      "Epoch 127/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0236 - acc: 0.9934 - val_loss: 0.4180 - val_acc: 0.9089\n",
      "Epoch 128/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0232 - acc: 0.9935 - val_loss: 0.4173 - val_acc: 0.9090\n",
      "Epoch 129/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0230 - acc: 0.9935 - val_loss: 0.4176 - val_acc: 0.9094\n",
      "Epoch 130/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0228 - acc: 0.9938 - val_loss: 0.4174 - val_acc: 0.9092\n",
      "Epoch 131/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0225 - acc: 0.9937 - val_loss: 0.4177 - val_acc: 0.9096\n",
      "Epoch 132/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0220 - acc: 0.9937 - val_loss: 0.4173 - val_acc: 0.9091\n",
      "Epoch 133/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0236 - acc: 0.9933 - val_loss: 0.4178 - val_acc: 0.9095\n",
      "Epoch 134/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0218 - acc: 0.9943 - val_loss: 0.4177 - val_acc: 0.9093\n",
      "Epoch 135/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0226 - acc: 0.9935 - val_loss: 0.4181 - val_acc: 0.9092\n",
      "Epoch 136/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0226 - acc: 0.9938 - val_loss: 0.4180 - val_acc: 0.9091\n",
      "Epoch 137/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0233 - acc: 0.9932 - val_loss: 0.4178 - val_acc: 0.9089\n",
      "Epoch 138/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0220 - acc: 0.9940 - val_loss: 0.4179 - val_acc: 0.9094\n",
      "Epoch 139/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0231 - acc: 0.9935 - val_loss: 0.4175 - val_acc: 0.9091\n",
      "Epoch 140/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0241 - acc: 0.9923 - val_loss: 0.4176 - val_acc: 0.9094\n",
      "Epoch 141/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0228 - acc: 0.9933 - val_loss: 0.4181 - val_acc: 0.9096\n",
      "Epoch 142/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0244 - acc: 0.9924 - val_loss: 0.4179 - val_acc: 0.9095\n",
      "Epoch 143/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0217 - acc: 0.9940 - val_loss: 0.4177 - val_acc: 0.9094\n",
      "Epoch 144/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0225 - acc: 0.9934 - val_loss: 0.4179 - val_acc: 0.9091\n",
      "Epoch 145/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0217 - acc: 0.9941 - val_loss: 0.4181 - val_acc: 0.9091\n",
      "Epoch 146/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0223 - acc: 0.9935 - val_loss: 0.4182 - val_acc: 0.9093\n",
      "Epoch 147/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0223 - acc: 0.9937 - val_loss: 0.4182 - val_acc: 0.9094\n",
      "Epoch 148/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0227 - acc: 0.9937 - val_loss: 0.4183 - val_acc: 0.9093\n",
      "Epoch 149/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0219 - acc: 0.9939 - val_loss: 0.4187 - val_acc: 0.9095\n",
      "Epoch 150/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0219 - acc: 0.9940 - val_loss: 0.4183 - val_acc: 0.9093\n",
      "Epoch 151/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0228 - acc: 0.9934 - val_loss: 0.4192 - val_acc: 0.9091\n",
      "Epoch 152/160\n",
      "390/390 [==============================] - 26s 68ms/step - loss: 0.0225 - acc: 0.9934 - val_loss: 0.4189 - val_acc: 0.9090\n",
      "Epoch 153/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.0230 - acc: 0.9936 - val_loss: 0.4191 - val_acc: 0.9089\n",
      "Epoch 154/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.0225 - acc: 0.9934 - val_loss: 0.4192 - val_acc: 0.9091\n",
      "Epoch 155/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0225 - acc: 0.9936 - val_loss: 0.4191 - val_acc: 0.9095\n",
      "Epoch 156/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0221 - acc: 0.9933 - val_loss: 0.4194 - val_acc: 0.9092\n",
      "Epoch 157/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0217 - acc: 0.9936 - val_loss: 0.4195 - val_acc: 0.9093\n",
      "Epoch 158/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.0227 - acc: 0.9932 - val_loss: 0.4199 - val_acc: 0.9087\n",
      "Epoch 159/160\n",
      "390/390 [==============================] - 26s 67ms/step - loss: 0.0225 - acc: 0.9933 - val_loss: 0.4192 - val_acc: 0.9086\n",
      "Epoch 160/160\n",
      "390/390 [==============================] - 26s 66ms/step - loss: 0.0232 - acc: 0.9928 - val_loss: 0.4201 - val_acc: 0.9093\n"
     ]
    }
   ],
   "source": [
    "history = m.fit(train_dataset, steps_per_epoch=390, epochs=160, validation_data=test_dataset, callbacks=[lr_schedule_cb, csv_logger_cb], validation_steps=78)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yO6f7GWdY11E"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "GGGFC-CIFAR10-Trainer.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
